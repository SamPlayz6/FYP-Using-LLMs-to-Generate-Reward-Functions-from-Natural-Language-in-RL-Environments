{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb227a0f-c945-4da9-8f83-ee73167700a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import scipy.stats as stats\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.formula.api import ols\n",
    "from tqdm.notebook import tqdm\n",
    "import inspect\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import auc\n",
    "from collections import deque\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "save_path = \"BipedalPerformanceResults\"\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Load Project Modules\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey, modelName\n",
    "from RLEnvironment.env import CustomBipedalWalkerEnv\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "\n",
    "# Load BipedalWalker specific reward functions\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.bipedalRewardFunctions import (\n",
    "    badRewardBipedal, \n",
    "    stabilityRewardBipedal,\n",
    "    efficiencyRewardBipedal,\n",
    "    potentialBasedRewardBipedal, \n",
    "    energyBasedRewardBipedal, \n",
    "    baselineRewardBipedal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d05f130-b3ee-4edc-9061-a4b264cd7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safely_get_function_source(func):\n",
    "    \"\"\"\n",
    "    Safely get the source code of a function, handling dynamically generated functions.\n",
    "    Returns a string representation if source code cannot be retrieved.\n",
    "    \"\"\"\n",
    "    # Handle the case when func is already a string\n",
    "    if isinstance(func, str):\n",
    "        return func\n",
    "        \n",
    "    try:\n",
    "        return inspect.getsource(func)\n",
    "    except (TypeError, OSError, IOError):\n",
    "        # For dynamically generated functions, create a representation\n",
    "        if hasattr(func, '__name__'):\n",
    "            return f\"<Dynamically generated function: {func.__name__}>\"\n",
    "        return f\"<Dynamically generated function: {func}>\"\n",
    "        \n",
    "def function_to_string(func):\n",
    "    \"\"\"\n",
    "    Convert a function to a string representation, suitable for display and comparison.\n",
    "    Handles various function types including lambdas, dynamically created functions, and strings.\n",
    "    \"\"\"\n",
    "    if func is None:\n",
    "        return \"None\"\n",
    "    \n",
    "    # Handle the case when func is already a string\n",
    "    if isinstance(func, str):\n",
    "        return func\n",
    "        \n",
    "    # Try to get the source code\n",
    "    try:\n",
    "        source = inspect.getsource(func)\n",
    "        return source.strip()\n",
    "    except (TypeError, OSError, IOError):\n",
    "        # If we can't get the source, create a detailed representation\n",
    "        if hasattr(func, '__name__'):\n",
    "            func_name = func.__name__\n",
    "        else:\n",
    "            func_name = str(func)\n",
    "            \n",
    "        # Get signature if possible\n",
    "        try:\n",
    "            signature = str(inspect.signature(func))\n",
    "        except (TypeError, ValueError):\n",
    "            signature = \"(unknown signature)\"\n",
    "            \n",
    "        # For simple functions, we can create a representation of their logic\n",
    "        try:\n",
    "            # Try to get the code object\n",
    "            code = func.__code__\n",
    "            # Include variable names and constants from the function\n",
    "            varnames = code.co_varnames[:code.co_argcount]\n",
    "            constants = code.co_consts\n",
    "            \n",
    "            return f\"Function {func_name}{signature} with {len(varnames)} arguments, {len(constants)} constants\"\n",
    "        except AttributeError:\n",
    "            # Fallback for non-Python functions or built-ins\n",
    "            return f\"Function {func_name}{signature} (source unavailable)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94626d79-17b6-4337-8f17-4d73cd2fb082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBipedalPerformanceTest(\n",
    "    episodes=5000, \n",
    "    changeInterval=1000, \n",
    "    legChanges=[0.3, 0.5],\n",
    "    terrainChanges=[0.0, 1.0],\n",
    "    seed=42,\n",
    "    collect_component_data=True\n",
    "):\n",
    "    \"\"\"Enhanced performance test for BipedalWalker with detailed adaptation metrics\"\"\"\n",
    "    print(f\"Starting BipedalWalker Performance Test with seed {seed}...\")\n",
    "    \n",
    "    # Set all random seeds\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    def create_fresh_env():\n",
    "        # Create fresh environment with consistent settings\n",
    "        env = gym.make('BipedalWalker-v3', render_mode=None)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        env.reset(seed=seed)\n",
    "        env = CustomBipedalWalkerEnv(env, numComponents=2)\n",
    "        env.setEnvironmentParameters(leg_length=legChanges[0], terrain_roughness=terrainChanges[0])\n",
    "        return env\n",
    "    \n",
    "    def create_dqn_agent(env, device):\n",
    "        return DQLearningAgent(\n",
    "            env=env, \n",
    "            stateSize=env.observation_space.shape[0], \n",
    "            actionSize=env.action_space.n,\n",
    "            device=device,\n",
    "            learningRate=0.0005,      # Lower for continuous actions\n",
    "            discountFactor=0.99,      # Standard value\n",
    "            epsilon=1.0,\n",
    "            epsilonDecay=0.9999,      # Slower decay for longer exploration\n",
    "            epsilonMin=0.1,           # Higher minimum for exploration\n",
    "            replayBufferSize=50000,   # Larger for complex environment\n",
    "            batchSize=64,             # Larger batch for stability\n",
    "            targetUpdateFreq=200      # Less frequent updates\n",
    "        )\n",
    "    \n",
    "    # Create initial environment\n",
    "    env = create_fresh_env()\n",
    "    \n",
    "    # Define reward function configurations\n",
    "    rewardfunctions = {\n",
    "        'adaptivereward': {\n",
    "            'agent': None,  # Will be created fresh for each test\n",
    "            'updatesystem': RewardUpdateSystem(apiKey, modelName),\n",
    "            'rewardfunction': None,\n",
    "            'update_method': 'llm'\n",
    "        },\n",
    "        'pbrs': {\n",
    "            'agent': None,\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': potentialBasedRewardBipedal,  # Change to match import\n",
    "            'update_method': None\n",
    "        },\n",
    "        'energy_based': {\n",
    "            'agent': None,\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': energyBasedRewardBipedal,  # Change to match import\n",
    "            'update_method': None\n",
    "        },\n",
    "        'baseline': {\n",
    "            'agent': None,\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': baselineRewardBipedal,  # Change to match import\n",
    "            'update_method': None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    test_order = ['adaptivereward', 'energy_based', 'baseline', 'pbrs']\n",
    "    \n",
    "    # Test each reward function\n",
    "    for rewardname in test_order:\n",
    "        print(f\"\\nTesting reward function: {rewardname}\")\n",
    "        \n",
    "        # Create fresh environment and agent\n",
    "        env = create_fresh_env()\n",
    "        rewardfunctions[rewardname]['agent'] = create_dqn_agent(env, device)\n",
    "        rewardinfo = rewardfunctions[rewardname]\n",
    "        \n",
    "        # Reset variables for this run\n",
    "        currentLegIdx = 0\n",
    "        currentTerrainIdx = 0\n",
    "        \n",
    "        # Set up the reward function\n",
    "        if rewardname == 'adaptivereward':\n",
    "            # Initialize both components for adaptive reward\n",
    "            env.setComponentReward(1, stabilityRewardBipedal)\n",
    "            env.setComponentReward(2, efficiencyRewardBipedal)\n",
    "            rewardinfo['updatesystem'].lastUpdateEpisode = 0\n",
    "        else:\n",
    "            env.setRewardFunction(rewardinfo['rewardfunction'])\n",
    "        \n",
    "        # Storage for episode data\n",
    "        episoderewards = []\n",
    "        episodedistances = []\n",
    "        rewardchangeepisodes = []\n",
    "        environmentchanges = []\n",
    "        \n",
    "        # Enhanced metrics collection\n",
    "        adaptation_metrics = {\n",
    "            'pre_change_performance': [],  # Performance before env change\n",
    "            'post_change_performance': [],  # Performance right after env change\n",
    "            'recovery_times': [],           # Episodes to recover after change\n",
    "            'performance_drop': [],         # Performance drop percentage\n",
    "            'change_episodes': []           # Episode numbers where changes occurred\n",
    "        }\n",
    "        \n",
    "        # Component weight tracking for adaptive reward only\n",
    "        component_weights = [] if rewardname == 'adaptivereward' and collect_component_data else None\n",
    "        component_updates = [] if rewardname == 'adaptivereward' and collect_component_data else None\n",
    "        \n",
    "        # Calculate rolling metrics function\n",
    "        def calculate_rolling_metrics(episode_data, window=20):\n",
    "            if len(episode_data) < window:\n",
    "                return np.mean(episode_data) if episode_data else 0\n",
    "            \n",
    "            return np.mean(episode_data[-window:])\n",
    "        \n",
    "        def onEpisodeEnd(env, updatesystem, episode, reward, steps, info=None):\n",
    "            nonlocal episoderewards, episodedistances, rewardchangeepisodes\n",
    "            nonlocal environmentchanges, currentLegIdx, currentTerrainIdx\n",
    "            nonlocal adaptation_metrics, component_weights, component_updates\n",
    "            \n",
    "            # Get distance traveled if available in info\n",
    "            distance = info.get('distance', 0) if info else 0\n",
    "            \n",
    "            # Record basic metrics\n",
    "            episoderewards.append(reward)\n",
    "            episodedistances.append(distance)\n",
    "            \n",
    "            # Calculate rolling metrics for decision making\n",
    "            metrics = {\n",
    "                'currentEpisode': episode,\n",
    "                'recentRewards': episoderewards[-100:] if len(episoderewards) > 100 else episoderewards,\n",
    "                'averageDistance': np.mean(episodedistances[-100:]) if episodedistances else 0,\n",
    "                'distanceVariance': np.var(episodedistances[-100:]) if len(episodedistances) > 1 else 0\n",
    "            }\n",
    "            \n",
    "            # Collect component weights for adaptive reward\n",
    "            if rewardname == 'adaptivereward' and collect_component_data and hasattr(env, 'getCurrentWeights'):\n",
    "                weights = env.getCurrentWeights()\n",
    "                component_weights.append({\n",
    "                    'episode': episode,\n",
    "                    'stability': weights['stability'],  # Use this key instead\n",
    "                    'efficiency': weights['efficiency'] # Use this key instead\n",
    "                })\n",
    "            \n",
    "            # Print debug info periodically\n",
    "            if episode % 500 == 0:\n",
    "                print(f\"\\nMetrics at Episode {episode}:\")\n",
    "                print(f\"Recent Average Reward: {np.mean(metrics['recentRewards']):.2f}\")\n",
    "                print(f\"Average Distance: {metrics['averageDistance']:.2f}\")\n",
    "                \n",
    "                if rewardname == 'adaptivereward' and hasattr(env, 'getCurrentWeights'):\n",
    "                    weights = env.getCurrentWeights()\n",
    "                    print(f\"Component Weights - Stability: {weights['stability']:.2f}, \"\n",
    "                        f\"Efficiency: {weights['efficiency']:.2f}\")\n",
    "            \n",
    "            # Handle LLM updates for adaptive reward\n",
    "            if rewardname == 'adaptivereward' and updatesystem is not None:\n",
    "                for component in range(1, 3):\n",
    "                    updatesystem.targetComponent = component\n",
    "                    if updatesystem.waitingTime(f'component_{component}', metrics, updatesystem.lastUpdateEpisode):\n",
    "                        current_func = env.rewardComponents[f'rewardFunction{component}']\n",
    "                        new_function, updated = updatesystem.validateAndUpdate(current_func)\n",
    "                        \n",
    "                        if updated:\n",
    "                            if isinstance(new_function, str):\n",
    "                                print(\"WARNING: New function returned as string, not function object\")\n",
    "                                continue  # Skip applying the update\n",
    "                            else:\n",
    "                                old_func_str = str(current_func)\n",
    "                                new_func_str = str(new_function)\n",
    "                            \n",
    "                            # Apply the update\n",
    "                            env.setComponentReward(component, new_function)\n",
    "                            rewardchangeepisodes.append(episode)\n",
    "                            updatesystem.lastUpdateEpisode = episode\n",
    "                            \n",
    "                            # Record the update details\n",
    "                            if component_updates is not None:\n",
    "                                component_updates.append({\n",
    "                                    'episode': episode,\n",
    "                                    'component': component,\n",
    "                                    'old_function': old_func_str,\n",
    "                                    'new_function': new_func_str,\n",
    "                                    'pre_update_performance': calculate_rolling_metrics(episoderewards[-20:]) if len(episoderewards) >= 20 else 0\n",
    "                                })\n",
    "                            \n",
    "                            print(f\"âœ“ LLM update for component {component} at episode {episode}\")\n",
    "            \n",
    "            # Environment changes for all approaches\n",
    "            if episode % changeInterval == 0 and episode > 0:\n",
    "                # Calculate pre-change performance (last 20 episodes)\n",
    "                pre_change_perf = calculate_rolling_metrics(episoderewards[-20:])\n",
    "                \n",
    "                # Alternate between leg length and terrain roughness changes\n",
    "                if episode % (changeInterval * 2) == 0:\n",
    "                    # Update leg length\n",
    "                    currentLegIdx = (currentLegIdx + 1) % len(legChanges)\n",
    "                    new_leg_length = legChanges[currentLegIdx]\n",
    "                    env.setEnvironmentParameters(leg_length=new_leg_length)\n",
    "                    print(f\"\\nChanged leg length to: {new_leg_length} at episode {episode}\")\n",
    "                else:\n",
    "                    # Update terrain roughness\n",
    "                    currentTerrainIdx = (currentTerrainIdx + 1) % len(terrainChanges)\n",
    "                    new_terrain = terrainChanges[currentTerrainIdx]\n",
    "                    env.setEnvironmentParameters(terrain_roughness=new_terrain)\n",
    "                    print(f\"\\nChanged terrain roughness to: {new_terrain} at episode {episode}\")\n",
    "                \n",
    "                # Record that a change happened\n",
    "                environmentchanges.append(episode)\n",
    "                \n",
    "                # Start tracking adaptation metrics for this change\n",
    "                adaptation_metrics['pre_change_performance'].append(pre_change_perf)\n",
    "                adaptation_metrics['change_episodes'].append(episode)\n",
    "                \n",
    "            # Track post-change performance and recovery\n",
    "            if environmentchanges and (episode - environmentchanges[-1]) == 20:  # 20 episodes after change\n",
    "                # Calculate performance drop\n",
    "                post_change_perf = calculate_rolling_metrics(episoderewards[-20:])\n",
    "                adaptation_metrics['post_change_performance'].append(post_change_perf)\n",
    "                \n",
    "                # Calculate performance drop as percentage\n",
    "                pre_perf = adaptation_metrics['pre_change_performance'][-1]\n",
    "                perf_drop_pct = (pre_perf - post_change_perf) / pre_perf * 100 if pre_perf > 0 else 0\n",
    "                adaptation_metrics['performance_drop'].append(perf_drop_pct)\n",
    "        \n",
    "        # Train the agent\n",
    "        agent, env, rewards = trainDQLearning(\n",
    "            agent=rewardinfo['agent'],\n",
    "            env=env,\n",
    "            numEpisodes=episodes,\n",
    "            updateSystem=rewardinfo['updatesystem'],\n",
    "            onEpisodeEnd=onEpisodeEnd\n",
    "        )\n",
    "        \n",
    "        # Calculate recovery times after training is complete\n",
    "        for i, change_ep in enumerate(adaptation_metrics['change_episodes']):\n",
    "            recovery_threshold = adaptation_metrics['pre_change_performance'][i] * 0.95\n",
    "            recovery_time = episodes - change_ep  # Default: never recovered\n",
    "            \n",
    "            # Find first point after change where performance exceeds recovery threshold\n",
    "            rolling_rewards = pd.Series(episoderewards[change_ep:]).rolling(window=20).mean()\n",
    "            for j, val in enumerate(rolling_rewards):\n",
    "                if not np.isnan(val) and val >= recovery_threshold:\n",
    "                    recovery_time = j\n",
    "                    break\n",
    "                    \n",
    "            adaptation_metrics['recovery_times'].append(recovery_time)\n",
    "        \n",
    "        # Store all results\n",
    "        results[rewardname] = {\n",
    "            'rewards': episoderewards,\n",
    "            'distances': episodedistances,\n",
    "            'rewardChanges': rewardchangeepisodes,\n",
    "            'environmentChanges': environmentchanges,\n",
    "            'adaptation_metrics': adaptation_metrics\n",
    "        }\n",
    "        \n",
    "        # Add component data for adaptive reward\n",
    "        if component_weights is not None:\n",
    "            results[rewardname]['component_weights'] = component_weights\n",
    "        \n",
    "        if component_updates is not None:\n",
    "            results[rewardname]['component_updates'] = component_updates\n",
    "        \n",
    "        # Print final metrics\n",
    "        print(f\"\\nCompleted testing {rewardname}\")\n",
    "        print(f\"Final average reward: {np.mean(episoderewards[-100:]):.2f}\")\n",
    "        print(f\"Final average distance: {np.mean(episodedistances[-100:]):.2f}\")\n",
    "        if adaptation_metrics['recovery_times']:\n",
    "            print(f\"Average recovery time: {np.mean(adaptation_metrics['recovery_times']):.2f} episodes\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3dc5c6-5fb1-4323-97cd-35e1a5dd5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statistical_tests(results):\n",
    "    \"\"\"Run statistical tests to compare performance of different reward approaches\"\"\"\n",
    "    # Dictionary to store test results\n",
    "    test_results = {}\n",
    "    \n",
    "    # Prepare data for tests\n",
    "    reward_data = []\n",
    "    \n",
    "    # Collect episode rewards for each approach\n",
    "    for reward_type, reward_info in results.items():\n",
    "        rewards = np.array(reward_info['rewards'])\n",
    "        distances = np.array(reward_info['distances'])\n",
    "        \n",
    "        # Create a DataFrame row for each episode\n",
    "        for i in range(len(rewards)):\n",
    "            row = {\n",
    "                'reward_type': reward_type,\n",
    "                'episode': i,\n",
    "                'reward': rewards[i],\n",
    "                'distance': distances[i] if i < len(distances) else 0\n",
    "            }\n",
    "            reward_data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(reward_data)\n",
    "    \n",
    "    # Run tests for different metrics\n",
    "    metrics = ['reward', 'distance']\n",
    "    for metric in metrics:\n",
    "        # Run ANOVA to test if there are significant differences between approaches\n",
    "        formula = f\"{metric} ~ C(reward_type)\"\n",
    "        model = ols(formula, data=df).fit()\n",
    "        anova_table = anova_lm(model, typ=2)\n",
    "        \n",
    "        # Store ANOVA results\n",
    "        p_value = anova_table['PR(>F)'][0]\n",
    "        significant = p_value < 0.05\n",
    "        \n",
    "        # Print ANOVA results\n",
    "        print(f\"\\nANOVA for {metric}:\")\n",
    "        print(anova_table)\n",
    "        print(f\"Significant difference: {significant} (p-value: {p_value:.4f})\")\n",
    "        \n",
    "        # If ANOVA is significant, run pairwise t-tests with Bonferroni correction\n",
    "        pairwise_results = {}\n",
    "        if significant:\n",
    "            # Get all unique reward types\n",
    "            reward_types = df['reward_type'].unique()\n",
    "            \n",
    "            # Calculate number of comparisons for Bonferroni correction\n",
    "            num_comparisons = len(reward_types) * (len(reward_types) - 1) // 2\n",
    "            # Bonferroni-corrected alpha\n",
    "            alpha_corrected = 0.05 / num_comparisons\n",
    "            \n",
    "            print(f\"\\nPairwise t-tests with Bonferroni correction (alpha = {alpha_corrected:.5f}):\")\n",
    "            \n",
    "            # Run t-tests for each pair of reward types\n",
    "            for i, type1 in enumerate(reward_types):\n",
    "                for type2 in reward_types[i+1:]:\n",
    "                    # Get data for each group\n",
    "                    group1_data = df[df['reward_type'] == type1][metric]\n",
    "                    group2_data = df[df['reward_type'] == type2][metric]\n",
    "                    \n",
    "                    # Run t-test\n",
    "                    t_stat, p_val = stats.ttest_ind(group1_data, group2_data, equal_var=False)\n",
    "                    \n",
    "                    # Check if significant with Bonferroni correction\n",
    "                    significant = p_val < alpha_corrected\n",
    "                    \n",
    "                    # Store results\n",
    "                    pair = f\"{type1} vs {type2}\"\n",
    "                    pairwise_results[pair] = {\n",
    "                        'mean_diff': float(group1_data.mean() - group2_data.mean()),\n",
    "                        'p_value': float(p_val),\n",
    "                        'significant': bool(significant),\n",
    "                        't_statistic': float(t_stat)\n",
    "                    }\n",
    "                    \n",
    "                    # Print formatted results\n",
    "                    sig_symbol = \"*\" if significant else \"\"\n",
    "                    print(f\"{pair}: diff = {pairwise_results[pair]['mean_diff']:.2f}, p = {p_val:.4f}{sig_symbol}\")\n",
    "        \n",
    "        # Store all results for this metric\n",
    "        test_results[metric] = {\n",
    "            'anova_p_value': float(p_value),\n",
    "            'anova_significant': bool(significant),\n",
    "            'pairwise_comparisons': pairwise_results\n",
    "        }\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47806a-7cae-4060-ba51-f2d12aeb28d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reward_over_time_plot(results, changeInterval, save_path=\".\", include_variance=True):\n",
    "    \"\"\"\n",
    "    Create a reward over time plot with optional variance bands.\n",
    "    Shows all reward functions on one plot with clear markers for environment changes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Define distinguishable colors and line styles\n",
    "    colors = {\n",
    "        'adaptivereward': '#1f77b4',  # blue\n",
    "        'energy_based': '#2ca02c',    # green\n",
    "        'pbrs': '#ff7f0e',            # orange\n",
    "        'baseline': '#d62728'         # red\n",
    "    }\n",
    "    \n",
    "    line_styles = {\n",
    "        'adaptivereward': '-',\n",
    "        'energy_based': '-.',\n",
    "        'pbrs': '--',\n",
    "        'baseline': ':'\n",
    "    }\n",
    "    \n",
    "    # Plot each reward function\n",
    "    for rewardname, rewardresults in results.items():\n",
    "        rewards = rewardresults['rewards']\n",
    "        \n",
    "        # Apply smoother with larger window for cleaner visualization\n",
    "        window = 50\n",
    "        smoothed_rewards = pd.Series(rewards).rolling(window=window, min_periods=1).mean()\n",
    "        \n",
    "        if include_variance:\n",
    "            # Calculate rolling standard deviation for variance bands\n",
    "            rolling_std = pd.Series(rewards).rolling(window=window, min_periods=1).std()\n",
    "            \n",
    "            # Create confidence interval (1 standard deviation)\n",
    "            upper_bound = smoothed_rewards + rolling_std\n",
    "            lower_bound = smoothed_rewards - rolling_std\n",
    "            \n",
    "            # Fill the area between upper and lower bounds\n",
    "            plt.fill_between(\n",
    "                range(len(rewards)),\n",
    "                lower_bound,\n",
    "                upper_bound,\n",
    "                alpha=0.2,\n",
    "                color=colors.get(rewardname, 'black'),\n",
    "                label=f\"{rewardname} variance\" if rewardname == list(results.keys())[0] else None\n",
    "            )\n",
    "        \n",
    "        plt.plot(\n",
    "            range(len(rewards)), \n",
    "            smoothed_rewards,\n",
    "            label=rewardname,\n",
    "            color=colors.get(rewardname, 'black'),\n",
    "            linestyle=line_styles.get(rewardname, '-'),\n",
    "            linewidth=2.5\n",
    "        )\n",
    "    \n",
    "    # Add vertical lines for environment changes\n",
    "    change_episodes = list(range(changeInterval, len(next(iter(results.values()))['rewards']), changeInterval))\n",
    "    for ep in change_episodes:\n",
    "        plt.axvline(x=ep, color='red', linestyle='--', alpha=0.5,\n",
    "                   label='Environment Change' if ep == change_episodes[0] else None)\n",
    "    \n",
    "    # Add annotations for environment changes\n",
    "    for i, ep in enumerate(change_episodes):\n",
    "        change_type = \"Leg Length\" if i % 2 == 0 else \"Terrain Roughness\"\n",
    "        plt.annotate(\n",
    "            f\"{change_type} Change\",\n",
    "            xy=(ep, plt.gca().get_ylim()[1] * 0.95),\n",
    "            xytext=(ep + 50, plt.gca().get_ylim()[1] * 0.95),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n",
    "            fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n",
    "        )\n",
    "    \n",
    "    # Add title and labels\n",
    "    title_suffix = \" with Variance Bands\" if include_variance else \"\"\n",
    "    plt.title(f'BipedalWalker Reward Performance Comparison{title_suffix}', fontsize=16)\n",
    "    plt.xlabel('Episode', fontsize=14)\n",
    "    plt.ylabel('Average Reward (smoothed)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Create a custom legend with larger markers\n",
    "    plt.legend(fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.15), \n",
    "               ncol=3, frameon=True, fancybox=True, shadow=True)\n",
    "    \n",
    "    # Adjust margins\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot if a path is provided\n",
    "    if save_path:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # Use a descriptive filename\n",
    "        variance_suffix = \"_with_variance\" if include_variance else \"\"\n",
    "        filename = f\"bipedal_reward_over_time{variance_suffix}.png\"\n",
    "        \n",
    "        # Join the path correctly\n",
    "        filepath = os.path.join(save_path, filename)\n",
    "        \n",
    "        # Print debug info\n",
    "        print(f\"Saving reward plot to: {filepath}\")\n",
    "        \n",
    "        # Save with a good margin setting\n",
    "        # Save and display approach\n",
    "        plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(f\"Saved plot to {filepath}\")\n",
    "    \n",
    "    return plt.gcf()  # Return the figure object\n",
    "\n",
    "def analyze_performance_breakdown(results, change_interval):\n",
    "    \"\"\"Analyze performance during different phases of the experiment\"\"\"\n",
    "    \n",
    "    # Identify environments\n",
    "    env_changes = next(iter(results.values()))['environmentChanges']\n",
    "    total_episodes = len(next(iter(results.values()))['rewards'])\n",
    "    \n",
    "    # Define environments/phases\n",
    "    phases = []\n",
    "    for i, change_ep in enumerate(env_changes):\n",
    "        start_ep = 0 if i == 0 else env_changes[i-1]\n",
    "        end_ep = change_ep\n",
    "        \n",
    "        # Add a phase\n",
    "        phases.append({\n",
    "            'name': f\"Environment {i+1}\",\n",
    "            'start': start_ep,\n",
    "            'end': end_ep,\n",
    "            'length': end_ep - start_ep\n",
    "        })\n",
    "    \n",
    "    # Add the final phase\n",
    "    if env_changes:\n",
    "        phases.append({\n",
    "            'name': f\"Environment {len(env_changes)+1}\",\n",
    "            'start': env_changes[-1],\n",
    "            'end': total_episodes,\n",
    "            'length': total_episodes - env_changes[-1]\n",
    "        })\n",
    "    \n",
    "    # Calculate per-environment metrics\n",
    "    phase_metrics = []\n",
    "    \n",
    "    for phase in phases:\n",
    "        phase_data = {'name': phase['name']}\n",
    "        \n",
    "        for reward_type, reward_data in results.items():\n",
    "            # Skip if this reward type doesn't have enough data\n",
    "            if phase['end'] > len(reward_data['rewards']):\n",
    "                continue\n",
    "                \n",
    "            # Calculate metrics for this phase\n",
    "            rewards_slice = reward_data['rewards'][phase['start']:phase['end']]\n",
    "            distance_slice = reward_data['distances'][phase['start']:phase['end']]\n",
    "            \n",
    "            # Skip empty slices\n",
    "            if not rewards_slice or not distance_slice:\n",
    "                continue\n",
    "                \n",
    "            phase_data[f\"{reward_type}_avg_reward\"] = np.mean(rewards_slice)\n",
    "            phase_data[f\"{reward_type}_avg_distance\"] = np.mean(distance_slice)\n",
    "            phase_data[f\"{reward_type}_stability\"] = 1.0 - (np.std(rewards_slice) / np.mean(rewards_slice)) if np.mean(rewards_slice) > 0 else 0\n",
    "        \n",
    "        phase_metrics.append(phase_data)\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    phase_df = pd.DataFrame(phase_metrics)\n",
    "    \n",
    "    print(\"\\nPerformance by Environment Phase:\")\n",
    "    print(phase_df.round(2))\n",
    "    \n",
    "    # Calculate relative performance (adaptive vs others)\n",
    "    for phase in phase_metrics:\n",
    "        # Skip if adaptive reward data is not available for this phase\n",
    "        if 'adaptivereward_avg_reward' not in phase:\n",
    "            continue\n",
    "            \n",
    "        for reward_type in [r for r in results.keys() if r != 'adaptivereward']:\n",
    "            # Skip if this reward type doesn't have data for this phase\n",
    "            if f\"{reward_type}_avg_reward\" not in phase:\n",
    "                continue\n",
    "                \n",
    "            # Calculate relative performance\n",
    "            relative_reward = (phase['adaptivereward_avg_reward'] / phase[f\"{reward_type}_avg_reward\"] - 1) * 100\n",
    "            phase[f\"relative_to_{reward_type}_pct\"] = relative_reward\n",
    "    \n",
    "    # Create a visualization of performance by phase\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Set up bar positions\n",
    "    bar_width = 0.2\n",
    "    index = np.arange(len(phase_metrics))\n",
    "    \n",
    "    # Plot bars for each reward type\n",
    "    colors = ['b', 'g', 'r', 'c']\n",
    "    for i, reward_type in enumerate(results.keys()):\n",
    "        values = [phase.get(f\"{reward_type}_avg_reward\", 0) for phase in phase_metrics]\n",
    "        plt.bar(index + i*bar_width, values, bar_width, \n",
    "               label=reward_type, color=colors[i], alpha=0.7)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Environment Phase')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('BipedalWalker Performance Comparison Across Environment Phases')\n",
    "    plt.xticks(index + bar_width, [phase['name'] for phase in phase_metrics])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"bipedal_performance_by_phase.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a heatmap of relative performance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Extract relative performance data\n",
    "    relative_data = []\n",
    "    for phase in phase_metrics:\n",
    "        row = {'phase': phase['name']}\n",
    "        for reward_type in [r for r in results.keys() if r != 'adaptivereward']:\n",
    "            key = f\"relative_to_{reward_type}_pct\"\n",
    "            if key in phase:\n",
    "                row[reward_type] = phase[key]\n",
    "        relative_data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    relative_df = pd.DataFrame(relative_data).set_index('phase')\n",
    "    \n",
    "    # Create heatmap\n",
    "    ax = sns.heatmap(relative_df, annot=True, cmap='RdYlGn', center=0, \n",
    "                    fmt='.1f', cbar_kws={'label': 'Relative Performance (%)'}, \n",
    "                    linewidths=0.5)\n",
    "    \n",
    "    plt.title('BipedalWalker Adaptive Reward Performance Relative to Other Approaches (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"bipedal_relative_performance_heatmap.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return phase_metrics\n",
    "\n",
    "def analyze_adaptive_components(results, save_path=\".\"):\n",
    "    \"\"\"Analyze the evolution and contribution of adaptive reward components\"\"\"\n",
    "    \n",
    "    # Check if adaptive reward results exist and have component data\n",
    "    if 'adaptivereward' not in results or 'component_weights' not in results['adaptivereward']:\n",
    "        print(\"No component data available for adaptive reward\")\n",
    "        return None\n",
    "    \n",
    "    # Extract component weight data\n",
    "    component_data = results['adaptivereward']['component_weights']\n",
    "    episodes = [d['episode'] for d in component_data]\n",
    "    balance_weights = [d['stability'] for d in component_data]\n",
    "    progress_weights = [d['efficiency'] for d in component_data]\n",
    "    \n",
    "    # Extract reward changes and environment changes\n",
    "    reward_changes = results['adaptivereward']['rewardChanges']\n",
    "    env_changes = results['adaptivereward']['environmentChanges']\n",
    "    \n",
    "    # Create weight evolution visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot component weights\n",
    "    plt.plot(episodes, balance_weights, 'b-', label='Balance Weight', linewidth=2)\n",
    "    plt.plot(episodes, progress_weights, 'g-', label='Progress Weight', linewidth=2)\n",
    "    \n",
    "    # Add vertical lines for reward function updates\n",
    "    for ep in reward_changes:\n",
    "        plt.axvline(x=ep, color='g', linestyle='--', alpha=0.5, \n",
    "                   label='Reward Update' if reward_changes.index(ep) == 0 else None)\n",
    "    \n",
    "    # Add vertical lines for environment changes\n",
    "    for ep in env_changes:\n",
    "        plt.axvline(x=ep, color='r', linestyle='--', alpha=0.5,\n",
    "                   label='Environment Change' if env_changes.index(ep) == 0 else None)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Evolution of BipedalWalker Adaptive Reward Component Weights')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Component Weight')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"bipedal_component_weight_evolution.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze performance correlation with weights\n",
    "    rewards = results['adaptivereward']['rewards']\n",
    "    \n",
    "    # Create a DataFrame with all data\n",
    "    df = pd.DataFrame({\n",
    "        'episode': episodes,\n",
    "        'balance_weight': balance_weights,\n",
    "        'progress_weight': progress_weights,\n",
    "        'reward': [rewards[ep] if ep < len(rewards) else np.nan for ep in episodes]\n",
    "    })\n",
    "    \n",
    "    # Calculate rolling reward for smoother analysis\n",
    "    df['rolling_reward'] = df['reward'].rolling(window=20).mean()\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr_balance = df['balance_weight'].corr(df['rolling_reward'])\n",
    "    corr_progress = df['progress_weight'].corr(df['rolling_reward'])\n",
    "    \n",
    "    print(f\"\\nCorrelation between balance weight and reward: {corr_balance:.3f}\")\n",
    "    print(f\"Correlation between progress weight and reward: {corr_progress:.3f}\")\n",
    "    \n",
    "    # Create a scatter plot of weights vs performance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot balance weight vs reward\n",
    "    sns.regplot(x='balance_weight', y='rolling_reward', data=df.dropna(), \n",
    "               ax=ax1, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n",
    "    ax1.set_title(f'Balance Weight vs Reward (corr = {corr_balance:.3f})')\n",
    "    ax1.set_xlabel('Balance Weight')\n",
    "    ax1.set_ylabel('Rolling Average Reward')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot progress weight vs reward\n",
    "    sns.regplot(x='progress_weight', y='rolling_reward', data=df.dropna(), \n",
    "               ax=ax2, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n",
    "    ax2.set_title(f'Progress Weight vs Reward (corr = {corr_progress:.3f})')\n",
    "    ax2.set_xlabel('Progress Weight')\n",
    "    ax2.set_ylabel('Rolling Average Reward')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"bipedal_component_weight_correlation.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze component updates if available\n",
    "    if 'component_updates' in results['adaptivereward'] and results['adaptivereward']['component_updates']:\n",
    "        updates = results['adaptivereward']['component_updates']\n",
    "        \n",
    "        print(\"\\nComponent Update Analysis:\")\n",
    "        print(f\"Total updates: {len(updates)}\")\n",
    "        \n",
    "        # Count updates by component\n",
    "        component_counts = {}\n",
    "        for update in updates:\n",
    "            component = f\"Component {update['component']}\"\n",
    "            if component not in component_counts:\n",
    "                component_counts[component] = 0\n",
    "            component_counts[component] += 1\n",
    "        \n",
    "        for component, count in component_counts.items():\n",
    "            print(f\"{component}: {count} updates\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_adaptation_speed(results, change_interval, save_path=\".\"):\n",
    "    \"\"\"\n",
    "    Analyze how quickly each reward approach adapts to environment changes.\n",
    "    \"\"\"\n",
    "    adaptation_metrics = {}\n",
    "    \n",
    "    for reward_type, reward_data in results.items():\n",
    "        # Skip if no rewards data\n",
    "        if 'rewards' not in reward_data or not reward_data['rewards']:\n",
    "            continue\n",
    "            \n",
    "        rewards = reward_data['rewards']\n",
    "        env_changes = reward_data['environmentChanges']\n",
    "        \n",
    "        # Skip if no environment changes\n",
    "        if not env_changes:\n",
    "            continue\n",
    "            \n",
    "        # Calculate metrics for each change\n",
    "        recovery_times = []\n",
    "        performance_drops = []\n",
    "        adaptation_effectiveness = []\n",
    "        \n",
    "        for change_ep in env_changes:\n",
    "            # Skip if change is too close to the end\n",
    "            if change_ep + 100 >= len(rewards):\n",
    "                continue\n",
    "                \n",
    "            # Calculate pre-change performance (20 episodes before change)\n",
    "            pre_change_start = max(0, change_ep - 20)\n",
    "            pre_change_rewards = rewards[pre_change_start:change_ep]\n",
    "            pre_change_avg = np.mean(pre_change_rewards) if pre_change_rewards else 0\n",
    "            \n",
    "            # Calculate immediate post-change performance (20 episodes after change)\n",
    "            post_change_rewards = rewards[change_ep:change_ep+20]\n",
    "            post_change_avg = np.mean(post_change_rewards) if post_change_rewards else 0\n",
    "            \n",
    "            # Calculate performance drop\n",
    "            if pre_change_avg > 0:\n",
    "                drop_pct = max(0, (pre_change_avg - post_change_avg) / pre_change_avg * 100)\n",
    "                performance_drops.append(drop_pct)\n",
    "            \n",
    "            # Calculate recovery time\n",
    "            recovery_threshold = 0.9 * pre_change_avg  # 90% of pre-change performance\n",
    "            recovery_ep = change_ep + change_interval  # Default: never recovered\n",
    "            \n",
    "            for i in range(change_ep, min(change_ep + change_interval, len(rewards))):\n",
    "                # Use sliding window of 10 episodes\n",
    "                window_start = max(change_ep, i - 10)\n",
    "                window_rewards = rewards[window_start:i+1]\n",
    "                window_avg = np.mean(window_rewards) if window_rewards else 0\n",
    "                \n",
    "                if window_avg >= recovery_threshold:\n",
    "                    recovery_ep = i\n",
    "                    break\n",
    "            \n",
    "            recovery_time = recovery_ep - change_ep\n",
    "            recovery_times.append(recovery_time)\n",
    "            \n",
    "            # Calculate adaptation effectiveness\n",
    "            if recovery_time > 0 and change_ep + recovery_time < len(rewards):\n",
    "                # Area under the recovery curve\n",
    "                recovery_rewards = rewards[change_ep:change_ep + recovery_time]\n",
    "                actual_area = np.sum(recovery_rewards)\n",
    "                ideal_area = pre_change_avg * recovery_time\n",
    "                \n",
    "                if ideal_area > 0:\n",
    "                    effectiveness = actual_area / ideal_area\n",
    "                    adaptation_effectiveness.append(effectiveness)\n",
    "        \n",
    "        # Store metrics for this reward type\n",
    "        adaptation_metrics[reward_type] = {\n",
    "            'avg_recovery_time': np.mean(recovery_times) if recovery_times else float('nan'),\n",
    "            'avg_performance_drop': np.mean(performance_drops) if performance_drops else float('nan'),\n",
    "            'avg_adaptation_effectiveness': np.mean(adaptation_effectiveness) if adaptation_effectiveness else float('nan'),\n",
    "            'recovery_times': recovery_times,\n",
    "            'performance_drops': performance_drops\n",
    "        }\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nBipedalWalker Adaptation Speed Analysis:\")\n",
    "    for reward_type, metrics in adaptation_metrics.items():\n",
    "        print(f\"\\n{reward_type}:\")\n",
    "        print(f\"  Average Recovery Time: {metrics['avg_recovery_time']:.2f} episodes\")\n",
    "        print(f\"  Average Performance Drop: {metrics['avg_performance_drop']:.2f}%\")\n",
    "        print(f\"  Adaptation Effectiveness: {metrics['avg_adaptation_effectiveness']:.2f}\")\n",
    "    \n",
    "    # Create visualization: Recovery Time Comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    labels = list(adaptation_metrics.keys())\n",
    "    recovery_times = [m['avg_recovery_time'] for m in adaptation_metrics.values()]\n",
    "    performance_drops = [m['avg_performance_drop'] for m in adaptation_metrics.values()]\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot recovery times\n",
    "    bars1 = ax1.bar(x - width/2, recovery_times, width, label='Recovery Time (episodes)', color='b', alpha=0.7)\n",
    "    ax1.set_ylabel('Recovery Time (episodes)', color='b')\n",
    "    ax1.tick_params(axis='y', colors='b')\n",
    "    \n",
    "    # Add second y-axis for performance drop\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x + width/2, performance_drops, width, label='Performance Drop (%)', color='r', alpha=0.7)\n",
    "    ax2.set_ylabel('Performance Drop (%)', color='r')\n",
    "    ax2.tick_params(axis='y', colors='r')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    ax1.set_title('BipedalWalker Adaptation Speed Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(labels)\n",
    "    ax1.set_xlabel('Reward Approach')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars1):\n",
    "        height = bar.get_height()\n",
    "        if not np.isnan(height):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{height:.1f}', ha='center', va='bottom', color='b', fontweight='bold')\n",
    "    \n",
    "    for i, bar in enumerate(bars2):\n",
    "        height = bar.get_height()\n",
    "        if not np.isnan(height):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom', color='r', fontweight='bold')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"bipedal_adaptation_speed_comparison.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Recovery Curves Visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get the first environment change episode\n",
    "    env_changes = next(iter(results.values()))['environmentChanges']\n",
    "    if not env_changes:\n",
    "        return adaptation_metrics\n",
    "    \n",
    "    first_change = env_changes[0]\n",
    "    window = 100  # Episodes to show after change\n",
    "    \n",
    "    # Plot recovery curves for each approach\n",
    "    for reward_type, reward_data in results.items():\n",
    "        if first_change >= len(reward_data['rewards']):\n",
    "            continue\n",
    "            \n",
    "        # Get pre-change performance as baseline\n",
    "        pre_change_start = max(0, first_change - 20)\n",
    "        pre_change_rewards = reward_data['rewards'][pre_change_start:first_change]\n",
    "        pre_change_avg = np.mean(pre_change_rewards) if pre_change_rewards else 1.0\n",
    "        \n",
    "        # Get post-change rewards\n",
    "        post_window = min(window, len(reward_data['rewards']) - first_change)\n",
    "        post_rewards = reward_data['rewards'][first_change:first_change + post_window]\n",
    "        \n",
    "        # Normalize as percentage of pre-change performance\n",
    "        normalized_rewards = [r / pre_change_avg * 100 for r in post_rewards]\n",
    "        \n",
    "        # Smooth the curve\n",
    "        smoothed = gaussian_filter1d(normalized_rewards, sigma=2)\n",
    "        \n",
    "        # Plot the recovery curve\n",
    "        plt.plot(range(len(smoothed)), smoothed, label=reward_type, linewidth=2)\n",
    "    \n",
    "    # Add pre-change level reference line\n",
    "    plt.axhline(y=100, color='k', linestyle='--', alpha=0.5, label='Pre-change level')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.title('BipedalWalker Recovery Curves After Environment Change')\n",
    "    plt.xlabel('Episodes After Change')\n",
    "    plt.ylabel('Performance (% of pre-change)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"bipedal_recovery_curves.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return adaptation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26496f-8504-400b-96c7-3848f7bb9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_bipedal_analysis(episodes=5000, change_interval=1000, num_runs=1):\n",
    "    \"\"\"Run a complete analysis of BipedalWalker performance with all metrics\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    # Create results directory\n",
    "    main_results_folder = \"BipedalPerformanceResults\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_folder = os.path.join(main_results_folder, f\"BipedalAnalysis_{timestamp}\")\n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n=============== Starting Run {run+1}/{num_runs} ===============\")\n",
    "        \n",
    "        # Run the performance test\n",
    "        seed = 42 + run\n",
    "        results = runBipedalPerformanceTest(\n",
    "            episodes=episodes,\n",
    "            changeInterval=change_interval,\n",
    "            legChanges=[0.3, 0.5],\n",
    "            terrainChanges=[0.0, 1.0],\n",
    "            seed=seed,\n",
    "            collect_component_data=True\n",
    "        )\n",
    "        \n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Create a subfolder for this run\n",
    "        run_folder = os.path.join(experiment_folder, f\"run_{run+1}\")\n",
    "        os.makedirs(run_folder, exist_ok=True)\n",
    "        \n",
    "        # Run analyses for this iteration\n",
    "        print(\"\\n--- Running Statistical Significance Tests ---\")\n",
    "        statistical_results = run_statistical_tests(results)\n",
    "        \n",
    "        print(\"\\n--- Analyzing Adaptation Speed ---\")\n",
    "        adaptation_metrics = analyze_adaptation_speed(results, change_interval, save_path=run_folder)\n",
    "        \n",
    "        print(\"\\n--- Analyzing Performance Breakdown ---\")\n",
    "        phase_metrics = analyze_performance_breakdown(results, change_interval)\n",
    "        \n",
    "        print(\"\\n--- Analyzing Adaptive Components ---\")\n",
    "        component_analysis = analyze_adaptive_components(results, save_path=run_folder)\n",
    "        \n",
    "        # Create reward over time plots\n",
    "        print(\"\\n--- Creating Reward Over Time Plots ---\")\n",
    "        reward_plot_with_variance = create_reward_over_time_plot(\n",
    "            results, \n",
    "            change_interval, \n",
    "            save_path=run_folder,\n",
    "            include_variance=True\n",
    "        )\n",
    "        plt.close(reward_plot_with_variance)\n",
    "        \n",
    "        reward_plot_without_variance = create_reward_over_time_plot(\n",
    "            results, \n",
    "            change_interval, \n",
    "            save_path=run_folder,\n",
    "            include_variance=False\n",
    "        )\n",
    "        plt.close(reward_plot_without_variance)\n",
    "        \n",
    "        # Save results to JSON\n",
    "        analysis_summary = {\n",
    "            'statistical_results': {\n",
    "                metric: {\n",
    "                    'anova_p_value': stats['anova_p_value'],\n",
    "                    'anova_significant': stats['anova_significant'],\n",
    "                    'pairwise_significant': {\n",
    "                        pair: result['significant'] \n",
    "                        for pair, result in stats['pairwise_comparisons'].items()\n",
    "                    }\n",
    "                }\n",
    "                for metric, stats in statistical_results.items()\n",
    "            },\n",
    "            'adaptation_metrics': {\n",
    "                reward_type: {\n",
    "                    'avg_recovery_time': metrics['avg_recovery_time'],\n",
    "                    'avg_performance_drop': metrics['avg_performance_drop']\n",
    "                }\n",
    "                for reward_type, metrics in adaptation_metrics.items()\n",
    "            },\n",
    "            'phase_metrics': [{\n",
    "                'name': phase['name'],\n",
    "                'best_approach': max([\n",
    "                    (reward_type, phase.get(f\"{reward_type}_avg_reward\", 0))\n",
    "                    for reward_type in results.keys()\n",
    "                ], key=lambda x: x[1])[0] if phase else None\n",
    "            } for phase in phase_metrics]\n",
    "        }\n",
    "        \n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        def convert_to_python_types(obj):\n",
    "            if isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_to_python_types(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_to_python_types(item) for item in obj]\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        with open(os.path.join(run_folder, 'bipedal_analysis_summary.json'), 'w') as f:\n",
    "            json.dump(convert_to_python_types(analysis_summary), f, indent=2)\n",
    "    \n",
    "    # If multiple runs, aggregate results\n",
    "    if num_runs > 1:\n",
    "        print(\"\\n=============== Aggregating Results Across Runs ===============\")\n",
    "        \n",
    "        # Create aggregate reward plot\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        # Define colors and line styles\n",
    "        colors = {\n",
    "            'adaptivereward': '#1f77b4',  # blue\n",
    "            'energy_based': '#2ca02c',    # green\n",
    "            'pbrs': '#ff7f0e',            # orange\n",
    "            'baseline': '#d62728'         # red\n",
    "        }\n",
    "        \n",
    "        line_styles = {\n",
    "            'adaptivereward': '-',\n",
    "            'energy_based': '-.',\n",
    "            'pbrs': '--',\n",
    "            'baseline': ':'\n",
    "        }\n",
    "        \n",
    "        # Organize data by reward type\n",
    "        reward_data = {}\n",
    "        max_episodes = 0\n",
    "        \n",
    "        # Collect data from all runs\n",
    "        for run_results in all_results:\n",
    "            for reward_type, results in run_results.items():\n",
    "                if reward_type not in reward_data:\n",
    "                    reward_data[reward_type] = []\n",
    "                \n",
    "                # Get reward data with smoothing\n",
    "                rewards = pd.Series(results['rewards']).rolling(window=100, min_periods=1).mean()\n",
    "                reward_data[reward_type].append(rewards)\n",
    "                max_episodes = max(max_episodes, len(rewards))\n",
    "        \n",
    "        # Plot each reward type with confidence interval\n",
    "        for reward_type, runs in reward_data.items():\n",
    "            # Make sure all runs have the same length\n",
    "            padded_runs = []\n",
    "            for run in runs:\n",
    "                if len(run) < max_episodes:\n",
    "                    padded = run.reindex(range(max_episodes), fill_value=np.nan)\n",
    "                else:\n",
    "                    padded = run\n",
    "                padded_runs.append(padded)\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            run_data = np.array([run.values for run in padded_runs])\n",
    "            \n",
    "            # Calculate mean and confidence intervals\n",
    "            mean_rewards = np.nanmean(run_data, axis=0)\n",
    "            if run_data.shape[0] > 1:\n",
    "                std_rewards = np.nanstd(run_data, axis=0)\n",
    "                ci = 1.96 * std_rewards / np.sqrt(run_data.shape[0])  # 95% CI\n",
    "            else:\n",
    "                ci = np.zeros_like(mean_rewards)\n",
    "            \n",
    "            # Plot mean line\n",
    "            plt.plot(\n",
    "                range(len(mean_rewards)),\n",
    "                mean_rewards,\n",
    "                label=reward_type,\n",
    "                color=colors.get(reward_type, 'black'),\n",
    "                linestyle=line_styles.get(reward_type, '-'),\n",
    "                linewidth=3\n",
    "            )\n",
    "            \n",
    "            # Plot confidence interval\n",
    "            plt.fill_between(\n",
    "                range(len(mean_rewards)),\n",
    "                mean_rewards - ci,\n",
    "                mean_rewards + ci,\n",
    "                color=colors.get(reward_type, 'black'),\n",
    "                alpha=0.2\n",
    "            )\n",
    "        \n",
    "        # Add vertical lines for environment changes\n",
    "        change_episodes = list(range(change_interval, max_episodes, change_interval))\n",
    "        for i, ep in enumerate(change_episodes):\n",
    "            # Skip if beyond our data\n",
    "            if ep >= max_episodes:\n",
    "                continue\n",
    "                \n",
    "            plt.axvline(\n",
    "                x=ep,\n",
    "                color='black',\n",
    "                linestyle='--',\n",
    "                alpha=0.5,\n",
    "                label='Environment Change' if i == 0 else None\n",
    "            )\n",
    "            \n",
    "            # Add annotation\n",
    "            change_type = \"Leg Length\" if i % 2 == 0 else \"Terrain Roughness\"\n",
    "            y_pos = plt.gca().get_ylim()[1] * 0.95\n",
    "            plt.annotate(\n",
    "                f\"{change_type} Change\",\n",
    "                xy=(ep, y_pos),\n",
    "                xytext=(ep + max_episodes * 0.02, y_pos),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n",
    "                fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n",
    "            )\n",
    "        \n",
    "        # Add title and labels\n",
    "        plt.title('BipedalWalker Aggregate Reward Performance Across All Runs (with 95% CI)', fontsize=18)\n",
    "        plt.xlabel('Episode', fontsize=16)\n",
    "        plt.ylabel('Average Reward (smoothed)', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Create a custom legend\n",
    "        plt.legend(\n",
    "            fontsize=14,\n",
    "            loc='upper center',\n",
    "            bbox_to_anchor=(0.5, -0.1),\n",
    "            ncol=4,\n",
    "            frameon=True,\n",
    "            fancybox=True,\n",
    "            shadow=True\n",
    "        )\n",
    "        \n",
    "        # Adjust margins\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        filepath = os.path.join(experiment_folder, \"bipedal_aggregate_reward_comparison.png\")\n",
    "        plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "        print(f\"Saved aggregate plot: bipedal_aggregate_reward_comparison.png in {experiment_folder}\")\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"\\nAnalysis complete. Results saved in {experiment_folder}\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edebbfca-5fdf-4f13-b58f-c1aab6ccc78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a complete analysis with smaller episodes for testing\n",
    "episodes = 50000\n",
    "change_interval = 25000\n",
    "num_runs = 2  # Set to more runs for better statistical power\n",
    "\n",
    "print(f\"Running complete BipedalWalker analysis with {num_runs} runs, {episodes} episodes and change interval {change_interval}\")\n",
    "\n",
    "# Run the complete analysis\n",
    "results = run_complete_bipedal_analysis(\n",
    "    episodes=episodes, \n",
    "    change_interval=change_interval, \n",
    "    num_runs=num_runs\n",
    ")\n",
    "\n",
    "print(\"\\nAnalysis complete - All functions have been executed:\")\n",
    "print(\"âœ“ run_statistical_tests\")\n",
    "print(\"âœ“ analyze_adaptation_speed\")\n",
    "print(\"âœ“ analyze_performance_breakdown\")\n",
    "print(\"âœ“ analyze_adaptive_components\")\n",
    "print(\"âœ“ create_reward_over_time_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd6cc4-0757-41cd-aaff-8a30c3594320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Conclusion and Key Findings for BipedalWalker Environment\n",
    "\n",
    "This analysis has provided detailed insights into the performance of adaptive reward functions in the \n",
    "more complex BipedalWalker environment:\n",
    "\n",
    "1. **Statistical Significance**: The differences between reward approaches were [significant/not significant] \n",
    "   with p-values of [values from your test results].\n",
    "\n",
    "2. **Adaptation Speed**: Adaptive reward functions demonstrated [faster/slower] recovery times after \n",
    "   environmental changes, with average recovery times of [X episodes] compared to [Y episodes] for \n",
    "   static approaches.\n",
    "\n",
    "3. **Performance Analysis**: \n",
    "   - Adaptive rewards showed particularly strong performance in [which environments]\n",
    "   - The terrain roughness changes had [more/less] impact than leg length changes\n",
    "   - Performance stabilized [quickly/slowly] after disruptions\n",
    "\n",
    "4. **Component Mechanism**: \n",
    "   - The adaptive approach adjusted component weights with [pattern observed]\n",
    "   - Balance components dominated in [which conditions]\n",
    "   - Progress components became more important in [which conditions]\n",
    "   - Component updates showed [what pattern] with environmental changes\n",
    "\n",
    "5. **Comparison to CartPole**:\n",
    "   - The BipedalWalker environment showed [similarities/differences] in adaptation patterns\n",
    "   - Reward function adaptation was [more/less] effective in this more complex environment\n",
    "   - Recovery times were [longer/shorter] than in CartPole experiments\n",
    "\n",
    "These findings [support/challenge] our hypothesis about adaptive reward functions in complex environments.\n",
    "The BipedalWalker experiments demonstrate that our approach [generalizes/struggles] when applied to \n",
    "higher-dimensional state and action spaces.\n",
    "\"\"\"\n",
    "\n",
    "# Create a summary table of key metrics to include in your paper\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Extract key metrics from results\n",
    "metrics_data = []\n",
    "for reward_type in ['adaptivereward', 'energy_based', 'baseline', 'pbrs']:\n",
    "    if reward_type in results[0]:\n",
    "        row = {\n",
    "            'Reward Approach': reward_type,\n",
    "            'Avg Reward': np.mean(results[0][reward_type]['rewards'][-100:]),\n",
    "            'Avg Distance': np.mean(results[0][reward_type]['distances'][-100:]),\n",
    "            'Recovery Time (eps)': results[0][reward_type]['adaptation_metrics'].get('avg_recovery_time', 'N/A'),\n",
    "            'Perf. Drop (%)': results[0][reward_type]['adaptation_metrics'].get('avg_performance_drop', 'N/A'),\n",
    "        }\n",
    "        metrics_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(metrics_data)\n",
    "display(HTML(summary_df.to_html(index=False)))\n",
    "\n",
    "# Also print in markdown format for easy inclusion in paper\n",
    "print(\"\\nMarkdown Table for Paper:\\n\")\n",
    "print(summary_df.to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
