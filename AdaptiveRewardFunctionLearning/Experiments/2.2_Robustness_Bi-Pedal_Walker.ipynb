{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7748f8a-2afb-4b52-8891-dfac12508463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from difflib import Differ\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Set paths\n",
    "current_dir = os.getcwd()\n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import required modules from existing codebase\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey, modelName\n",
    "from RLEnvironment.env import CustomBipedalWalkerEnv\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.bipedalRewardFunctions import (\n",
    "    badRewardBipedal, stabilityRewardBipedal, efficiencyRewardBipedal,\n",
    "    potentialBasedRewardBipedal, energyBasedRewardBipedal, baselineRewardBipedal\n",
    ")\n",
    "\n",
    "# Helper function to save plots\n",
    "def save_plot(fig, name, folder=\"BipedalWalkerRobustnessResults\"):\n",
    "    # Create logs directory with subfolder if it doesn't exist\n",
    "    logs_dir = Path(project_root) / 'AdaptiveRewardFunctionLearning' / 'Experiments' / folder\n",
    "    if not os.path.exists(logs_dir):\n",
    "        os.makedirs(logs_dir)\n",
    "    \n",
    "    # Create timestamp\n",
    "    timestamp = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "    \n",
    "    # Full path for saving\n",
    "    filepath = os.path.join(logs_dir, f\"{name}_{timestamp}.png\")\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot: {name}_{timestamp}.png in {folder}\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c35882-4d30-4d8e-8673-5d94f8f04b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBipedalRobustnessTest(episodes=5000, update_interval=1000, \n",
    "                            leg_changes=[30, 50],\n",
    "                            terrain_roughness=1.0,\n",
    "                            gravity=9.8,\n",
    "                            seed=42,\n",
    "                            discretize_bins=3):\n",
    "    \"\"\"\n",
    "    Run a robustness test for the BipedalWalker environment.\n",
    "    \n",
    "    Args:\n",
    "        episodes: Number of training episodes\n",
    "        update_interval: Episodes between environment changes\n",
    "        leg_changes: List of leg lengths to alternate between\n",
    "        terrain_roughness: Terrain roughness parameter\n",
    "        gravity: Gravity parameter\n",
    "        seed: Random seed\n",
    "        discretize_bins: Number of bins for discretizing continuous actions\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results of the experiment\n",
    "    \"\"\"\n",
    "    print(f\"Starting BipedalWalker Robustness Test with seed {seed}...\")\n",
    "    \n",
    "    # Set random seeds\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make('BipedalWalker-v3', hardcore=False, render_mode=None)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.reset(seed=seed)\n",
    "    \n",
    "    # Wrap with custom environment\n",
    "    env = CustomBipedalWalkerEnv(env, numComponents=2, discretize_bins=discretize_bins)\n",
    "    \n",
    "    # Set initial parameters\n",
    "    env.setEnvironmentParameters(\n",
    "        leg_length=leg_changes[0],\n",
    "        terrain_roughness=terrain_roughness,\n",
    "        gravity=gravity\n",
    "    )\n",
    "    \n",
    "    # Create agent - works with discretized action space\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n  # Discretized action space\n",
    "    \n",
    "    agent = DQLearningAgent(\n",
    "        env=env, \n",
    "        stateSize=state_size, \n",
    "        actionSize=action_size, \n",
    "        device=device,\n",
    "        learningRate=0.0005,\n",
    "        discountFactor=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilonDecay=0.9995,\n",
    "        epsilonMin=0.05,\n",
    "        replayBufferSize=100000,\n",
    "        batchSize=64,\n",
    "        targetUpdateFreq=200\n",
    "    )\n",
    "    \n",
    "    # Create update system\n",
    "    update_system = RewardUpdateSystem(apiKey, modelName)\n",
    "    \n",
    "    # Start with bad reward function\n",
    "    env.setRewardFunction(badRewardBipedal)\n",
    "    \n",
    "    # Initialize components (to be activated later)\n",
    "    env.setComponentReward(1, stabilityRewardBipedal)\n",
    "    env.setComponentReward(2, efficiencyRewardBipedal)\n",
    "    update_system.lastUpdateEpisode = 0\n",
    "    \n",
    "    # Storage for episode data\n",
    "    episode_rewards = []\n",
    "    episode_distances = []  # For BipedalWalker, track distance instead of balance time\n",
    "    reward_change_episodes = []\n",
    "    environment_changes = []\n",
    "    \n",
    "    # Component weight tracking\n",
    "    component_weights = []\n",
    "    component_updates = []\n",
    "    \n",
    "    # Parameter index tracking\n",
    "    current_param_idx = 0\n",
    "    \n",
    "    def onEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "        nonlocal episode_rewards, episode_distances, reward_change_episodes, environment_changes\n",
    "        nonlocal current_param_idx, component_weights, component_updates\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        # For BipedalWalker, track forward distance\n",
    "        if hasattr(env.env.unwrapped, 'hull'):\n",
    "            distance = env.env.unwrapped.hull.position[0]\n",
    "        else:\n",
    "            distance = 0  # Fallback\n",
    "        episode_distances.append(distance)\n",
    "        \n",
    "        # Calculate metrics for decision making\n",
    "        metrics = {\n",
    "            'currentEpisode': episode,\n",
    "            'recentRewards': episode_rewards[-100:] if len(episode_rewards) > 100 else episode_rewards,\n",
    "            'averageDistance': np.mean(episode_distances[-100:]) if episode_distances else 0,\n",
    "            'distanceVariance': np.var(episode_distances[-100:]) if len(episode_distances) > 1 else 0\n",
    "        }\n",
    "        \n",
    "        # Collect component weights\n",
    "        if hasattr(env, 'getCurrentWeights'):\n",
    "            weights = env.getCurrentWeights()\n",
    "            component_weights.append({\n",
    "                'episode': episode,\n",
    "                'stability': weights['stability'],\n",
    "                'efficiency': weights['efficiency']\n",
    "            })\n",
    "        \n",
    "        # Print debug info periodically\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"\\nMetrics at Episode {episode}:\")\n",
    "            print(f\"Recent Average Reward: {np.mean(metrics['recentRewards']):.2f}\")\n",
    "            print(f\"Average Distance: {metrics['averageDistance']:.2f}\")\n",
    "            \n",
    "            if hasattr(env, 'getCurrentWeights'):\n",
    "                weights = env.getCurrentWeights()\n",
    "                print(f\"Component Weights - Stability: {weights['stability']:.2f}, \"\n",
    "                      f\"Efficiency: {weights['efficiency']:.2f}\")\n",
    "        \n",
    "        # Transition from bad reward to component-based reward at episode 500\n",
    "        if episode == 5000:\n",
    "            print(\"\\nTransitioning from bad reward to component-based reward\")\n",
    "            env.usingComponents = True  # Activate component-based reward\n",
    "            reward_change_episodes.append(episode)\n",
    "        \n",
    "        # Handle LLM updates for adaptive reward after transition\n",
    "        if updatesystem is not None and episode > 5000:\n",
    "            for component in range(1, 3):\n",
    "                updatesystem.targetComponent = component\n",
    "                if updatesystem.waitingTime(f'component_{component}', metrics, updatesystem.lastUpdateEpisode):\n",
    "                    current_func = env.rewardComponents[f'rewardFunction{component}']\n",
    "                    new_function, updated = updatesystem.validateAndUpdate(current_func)\n",
    "                    \n",
    "                    if updated:\n",
    "                        # Record old function string\n",
    "                        try:\n",
    "                            old_func_str = inspect.getsource(current_func)\n",
    "                        except:\n",
    "                            old_func_str = str(current_func)\n",
    "                        \n",
    "                        # Record new function string\n",
    "                        if callable(new_function):\n",
    "                            try:\n",
    "                                new_func_str = inspect.getsource(new_function)\n",
    "                            except:\n",
    "                                new_func_str = str(new_function)\n",
    "                        else:\n",
    "                            new_func_str = str(new_function)\n",
    "                        \n",
    "                        # Apply the update\n",
    "                        env.setComponentReward(component, new_function)\n",
    "                        reward_change_episodes.append(episode)\n",
    "                        updatesystem.lastUpdateEpisode = episode\n",
    "                        \n",
    "                        # Record update details\n",
    "                        component_updates.append({\n",
    "                            'episode': episode,\n",
    "                            'component': component,\n",
    "                            'old_function': old_func_str,\n",
    "                            'new_function': new_func_str,\n",
    "                            'pre_update_performance': np.mean(episode_rewards[-20:]) if len(episode_rewards) >= 20 else 0\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"✓ LLM update for component {component} at episode {episode}\")\n",
    "        \n",
    "        # Environment changes at specified intervals\n",
    "        if episode % update_interval == 0 and episode > 0:\n",
    "            current_param_idx = (current_param_idx + 1) % len(leg_changes)\n",
    "            new_leg_length = leg_changes[current_param_idx]\n",
    "            \n",
    "            # Apply new parameters\n",
    "            env.setEnvironmentParameters(\n",
    "                leg_length=new_leg_length,\n",
    "                terrain_roughness=terrain_roughness,\n",
    "                gravity=gravity\n",
    "            )\n",
    "            \n",
    "            # Record change\n",
    "            environment_changes.append(episode)\n",
    "            print(f\"\\nChanged leg length to: {new_leg_length} at episode {episode}\")\n",
    "    \n",
    "    # Train the agent\n",
    "    agent, env, rewards = trainDQLearning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        numEpisodes=episodes,\n",
    "        updateSystem=update_system,\n",
    "        onEpisodeEnd=onEpisodeEnd\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'adaptivereward': {\n",
    "            'rewards': episode_rewards,\n",
    "            'distances': episode_distances,\n",
    "            'rewardChanges': reward_change_episodes,\n",
    "            'environmentChanges': environment_changes,\n",
    "            'component_weights': component_weights,\n",
    "            'component_updates': component_updates\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(f\"\\nCompleted testing adaptive reward\")\n",
    "    print(f\"Final average reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "    print(f\"Final average distance: {np.mean(episode_distances[-100:]):.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b9298-acd8-4265-82cc-ff5279741742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRewardUpdateExperiment(episodes=5000, update_interval=1000, seed=42, discretize_bins=3):\n",
    "    \"\"\"\n",
    "    Run an experiment to test the effectiveness of LLM-based reward function updates\n",
    "    without environmental changes.\n",
    "    \n",
    "    Args:\n",
    "        episodes: Number of training episodes\n",
    "        update_interval: Episodes between forced reward function updates\n",
    "        seed: Random seed\n",
    "        discretize_bins: Number of bins for discretizing continuous actions\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results of the experiment including rewards, distances, and update points\n",
    "    \"\"\"\n",
    "    print(f\"Starting BipedalWalker Reward Update Experiment with seed {seed}...\")\n",
    "    \n",
    "    # Set random seeds\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make('BipedalWalker-v3', hardcore=False, render_mode=None)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.reset(seed=seed)\n",
    "    \n",
    "    # Wrap with custom environment\n",
    "    env = CustomBipedalWalkerEnv(env, numComponents=2, discretize_bins=discretize_bins)\n",
    "    \n",
    "    # Set fixed parameters - no environmental changes\n",
    "    env.setEnvironmentParameters(\n",
    "        leg_length=40,  # Medium length\n",
    "        terrain_roughness=0.5,  # Moderate terrain\n",
    "        gravity=9.8\n",
    "    )\n",
    "    \n",
    "    # Create agent\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n  # Discretized action space\n",
    "    \n",
    "    agent = DQLearningAgent(\n",
    "        env=env, \n",
    "        stateSize=state_size, \n",
    "        actionSize=action_size, \n",
    "        device=device,\n",
    "        learningRate=0.0005,\n",
    "        discountFactor=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilonDecay=0.9995,\n",
    "        epsilonMin=0.05,\n",
    "        replayBufferSize=100000,\n",
    "        batchSize=64,\n",
    "        targetUpdateFreq=200\n",
    "    )\n",
    "    \n",
    "    # Create update system\n",
    "    update_system = RewardUpdateSystem(apiKey, modelName)\n",
    "    \n",
    "    # Initialize with component rewards\n",
    "    env.setComponentReward(1, stabilityRewardBipedal)\n",
    "    env.setComponentReward(2, efficiencyRewardBipedal)\n",
    "    update_system.lastUpdateEpisode = 0\n",
    "    \n",
    "    # Storage for episode data\n",
    "    episode_rewards = []\n",
    "    episode_distances = []\n",
    "    reward_change_episodes = []\n",
    "    \n",
    "    # Component weight tracking\n",
    "    component_weights = []\n",
    "    component_updates = []\n",
    "    \n",
    "    # For tracking performance before/after updates\n",
    "    update_performance = []\n",
    "    \n",
    "    def onEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "        nonlocal episode_rewards, episode_distances, reward_change_episodes\n",
    "        nonlocal component_weights, component_updates, update_performance\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        # For BipedalWalker, track forward distance\n",
    "        if hasattr(env.env.unwrapped, 'hull'):\n",
    "            distance = env.env.unwrapped.hull.position[0]\n",
    "        else:\n",
    "            distance = 0  # Fallback\n",
    "        episode_distances.append(distance)\n",
    "        \n",
    "        # Calculate metrics for decision making\n",
    "        metrics = {\n",
    "            'currentEpisode': episode,\n",
    "            'recentRewards': episode_rewards[-100:] if len(episode_rewards) > 100 else episode_rewards,\n",
    "            'averageDistance': np.mean(episode_distances[-100:]) if episode_distances else 0,\n",
    "            'distanceVariance': np.var(episode_distances[-100:]) if len(episode_distances) > 1 else 0\n",
    "        }\n",
    "        \n",
    "        # Collect component weights\n",
    "        if hasattr(env, 'getCurrentWeights'):\n",
    "            weights = env.getCurrentWeights()\n",
    "            component_weights.append({\n",
    "                'episode': episode,\n",
    "                'stability': weights['stability'],\n",
    "                'efficiency': weights['efficiency']\n",
    "            })\n",
    "        \n",
    "        # Print debug info periodically\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"\\nMetrics at Episode {episode}:\")\n",
    "            print(f\"Recent Average Reward: {np.mean(metrics['recentRewards']):.2f}\")\n",
    "            print(f\"Average Distance: {metrics['averageDistance']:.2f}\")\n",
    "            \n",
    "            if hasattr(env, 'getCurrentWeights'):\n",
    "                weights = env.getCurrentWeights()\n",
    "                print(f\"Component Weights - Stability: {weights['stability']:.2f}, \"\n",
    "                      f\"Efficiency: {weights['efficiency']:.2f}\")\n",
    "        \n",
    "        # Forced reward function updates at specified intervals\n",
    "        forced_update = episode % update_interval == 0 and episode > 0\n",
    "        \n",
    "        # Handle LLM updates for reward function\n",
    "        if updatesystem is not None and (forced_update or updatesystem.waitingTime('adaptive', metrics, updatesystem.lastUpdateEpisode)):\n",
    "            # Before update metrics\n",
    "            pre_update_rewards = episode_rewards[-20:] if len(episode_rewards) >= 20 else episode_rewards\n",
    "            pre_update_distances = episode_distances[-20:] if len(episode_distances) >= 20 else episode_distances\n",
    "            \n",
    "            pre_update_performance = {\n",
    "                'episode': episode,\n",
    "                'pre_avg_reward': np.mean(pre_update_rewards),\n",
    "                'pre_avg_distance': np.mean(pre_update_distances)\n",
    "            }\n",
    "            \n",
    "            # Perform updates for both components\n",
    "            for component in range(1, 3):\n",
    "                updatesystem.targetComponent = component\n",
    "                current_func = env.rewardComponents[f'rewardFunction{component}']\n",
    "                new_function, updated = updatesystem.validateAndUpdate(current_func)\n",
    "                \n",
    "                if updated:\n",
    "                    # Record old function string\n",
    "                    try:\n",
    "                        old_func_str = inspect.getsource(current_func)\n",
    "                    except:\n",
    "                        old_func_str = str(current_func)\n",
    "                    \n",
    "                    # Record new function string\n",
    "                    if callable(new_function):\n",
    "                        try:\n",
    "                            new_func_str = inspect.getsource(new_function)\n",
    "                        except:\n",
    "                            new_func_str = str(new_function)\n",
    "                    else:\n",
    "                        new_func_str = str(new_function)\n",
    "                    \n",
    "                    # Apply the update\n",
    "                    env.setComponentReward(component, new_function)\n",
    "                    reward_change_episodes.append(episode)\n",
    "                    updatesystem.lastUpdateEpisode = episode\n",
    "                    \n",
    "                    # Record update details\n",
    "                    component_updates.append({\n",
    "                        'episode': episode,\n",
    "                        'component': component,\n",
    "                        'old_function': old_func_str,\n",
    "                        'new_function': new_func_str\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"✓ LLM update for component {component} at episode {episode}\")\n",
    "            \n",
    "            # Only record performance metrics if any component was updated\n",
    "            if episode in reward_change_episodes:\n",
    "                update_performance.append(pre_update_performance)\n",
    "    \n",
    "    # Train the agent\n",
    "    agent, env, rewards = trainDQLearning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        numEpisodes=episodes,\n",
    "        updateSystem=update_system,\n",
    "        onEpisodeEnd=onEpisodeEnd\n",
    "    )\n",
    "    \n",
    "    # Calculate post-update performance for each update\n",
    "    for i, update in enumerate(update_performance):\n",
    "        update_ep = update['episode']\n",
    "        \n",
    "        # Look ahead 20 episodes after the update\n",
    "        end_idx = min(update_ep + 20, len(episode_rewards))\n",
    "        post_update_rewards = episode_rewards[update_ep:end_idx]\n",
    "        post_update_distances = episode_distances[update_ep:end_idx]\n",
    "        \n",
    "        update_performance[i]['post_avg_reward'] = np.mean(post_update_rewards)\n",
    "        update_performance[i]['post_avg_distance'] = np.mean(post_update_distances)\n",
    "        update_performance[i]['reward_change_pct'] = ((update_performance[i]['post_avg_reward'] / update_performance[i]['pre_avg_reward']) - 1) * 100 if update_performance[i]['pre_avg_reward'] > 0 else 0\n",
    "        update_performance[i]['distance_change_pct'] = ((update_performance[i]['post_avg_distance'] / update_performance[i]['pre_avg_distance']) - 1) * 100 if update_performance[i]['pre_avg_distance'] > 0 else 0\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'rewards': episode_rewards,\n",
    "        'distances': episode_distances,\n",
    "        'reward_changes': reward_change_episodes,\n",
    "        'component_weights': component_weights,\n",
    "        'component_updates': component_updates,\n",
    "        'update_performance': update_performance\n",
    "    }\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(f\"\\nCompleted reward update experiment\")\n",
    "    print(f\"Final average reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "    print(f\"Final average distance: {np.mean(episode_distances[-100:]):.2f}\")\n",
    "    print(f\"Number of reward function updates: {len(reward_change_episodes)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7eced-c006-4ae7-bddd-b8c811e28bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAdaptationComparisonExperiment(episodes=2000, change_episode=1000, post_change_episodes=2000, seed=42, discretize_bins=3):\n",
    "    \"\"\"\n",
    "    Run a comparative experiment with an environmental change, comparing adaptive\n",
    "    and static reward functions.\n",
    "    \n",
    "    Args:\n",
    "        episodes: Number of initial training episodes before the split\n",
    "        change_episode: Episode at which to make the environmental change\n",
    "        post_change_episodes: Number of episodes to run after the change\n",
    "        seed: Random seed\n",
    "        discretize_bins: Number of bins for discretizing continuous actions\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results of both branches of the experiment\n",
    "    \"\"\"\n",
    "    print(f\"Starting BipedalWalker Adaptation Comparison Experiment with seed {seed}...\")\n",
    "    \n",
    "    # Set random seeds\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make('BipedalWalker-v3', hardcore=False, render_mode=None)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.reset(seed=seed)\n",
    "    \n",
    "    # Wrap with custom environment\n",
    "    env = CustomBipedalWalkerEnv(env, numComponents=2, discretize_bins=discretize_bins)\n",
    "    \n",
    "    # Set initial parameters\n",
    "    initial_leg_length = 30  # Start with short legs\n",
    "    changed_leg_length = 50  # Change to long legs\n",
    "    \n",
    "    env.setEnvironmentParameters(\n",
    "        leg_length=initial_leg_length,\n",
    "        terrain_roughness=0.5,  # Moderate terrain\n",
    "        gravity=9.8\n",
    "    )\n",
    "    \n",
    "    # Create agent\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n  # Discretized action space\n",
    "    \n",
    "    agent = DQLearningAgent(\n",
    "        env=env, \n",
    "        stateSize=state_size, \n",
    "        actionSize=action_size, \n",
    "        device=device,\n",
    "        learningRate=0.0005,\n",
    "        discountFactor=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilonDecay=0.9995,\n",
    "        epsilonMin=0.05,\n",
    "        replayBufferSize=100000,\n",
    "        batchSize=64,\n",
    "        targetUpdateFreq=200\n",
    "    )\n",
    "    \n",
    "    # Create update system\n",
    "    update_system = RewardUpdateSystem(apiKey, modelName)\n",
    "    \n",
    "    # Initialize with component rewards\n",
    "    env.setComponentReward(1, stabilityRewardBipedal)\n",
    "    env.setComponentReward(2, efficiencyRewardBipedal)\n",
    "    update_system.lastUpdateEpisode = 0\n",
    "    \n",
    "    # Storage for episode data\n",
    "    episode_rewards = []\n",
    "    episode_distances = []\n",
    "    \n",
    "    # Component weight tracking\n",
    "    component_weights = []\n",
    "    \n",
    "    def onEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "        nonlocal episode_rewards, episode_distances, component_weights\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        # For BipedalWalker, track forward distance\n",
    "        if hasattr(env.env.unwrapped, 'hull'):\n",
    "            distance = env.env.unwrapped.hull.position[0]\n",
    "        else:\n",
    "            distance = 0  # Fallback\n",
    "        episode_distances.append(distance)\n",
    "        \n",
    "        # Collect component weights\n",
    "        if hasattr(env, 'getCurrentWeights'):\n",
    "            weights = env.getCurrentWeights()\n",
    "            component_weights.append({\n",
    "                'episode': episode,\n",
    "                'stability': weights['stability'],\n",
    "                'efficiency': weights['efficiency']\n",
    "            })\n",
    "        \n",
    "        # Print debug info periodically\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"\\nMetrics at Episode {episode}:\")\n",
    "            print(f\"Recent Average Reward: {np.mean(episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards):.2f}\")\n",
    "            print(f\"Average Distance: {np.mean(episode_distances[-100:] if len(episode_distances) >= 100 else episode_distances):.2f}\")\n",
    "            \n",
    "            if hasattr(env, 'getCurrentWeights'):\n",
    "                weights = env.getCurrentWeights()\n",
    "                print(f\"Component Weights - Stability: {weights['stability']:.2f}, \"\n",
    "                      f\"Efficiency: {weights['efficiency']:.2f}\")\n",
    "    \n",
    "    # Train the agent for initial episodes\n",
    "    print(f\"\\nInitial training phase with leg length {initial_leg_length}...\")\n",
    "    agent, env, _ = trainDQLearning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        numEpisodes=episodes,\n",
    "        updateSystem=update_system,\n",
    "        onEpisodeEnd=onEpisodeEnd\n",
    "    )\n",
    "    \n",
    "    # Save the state at the split point\n",
    "    print(f\"\\nSaving state at episode {episodes} for branching...\")\n",
    "    \n",
    "    # Save important state information\n",
    "    model_state = copy.deepcopy(agent.model.state_dict())\n",
    "    target_model_state = copy.deepcopy(agent.targetModel.state_dict())\n",
    "    memory = copy.deepcopy(agent.memory)\n",
    "    epsilon = agent.epsilon\n",
    "    \n",
    "    # Save environment reward components and weights\n",
    "    reward_components = {}\n",
    "    component_weights_save = {}\n",
    "    for i in range(1, 3):\n",
    "        func_name = f'rewardFunction{i}'\n",
    "        if func_name in env.rewardComponents:\n",
    "            reward_components[func_name] = env.rewardComponents[func_name]\n",
    "    \n",
    "    if hasattr(env, 'componentWeights'):\n",
    "        component_weights_save = copy.deepcopy(env.componentWeights)\n",
    "    \n",
    "    # Create results containers for both branches\n",
    "    adaptive_results = {\n",
    "        'rewards': copy.deepcopy(episode_rewards),\n",
    "        'distances': copy.deepcopy(episode_distances),\n",
    "        'component_weights': copy.deepcopy(component_weights),\n",
    "        'reward_changes': [],\n",
    "        'environment_changes': [episodes]  # The change happens right after the initial training\n",
    "    }\n",
    "    \n",
    "    static_results = {\n",
    "        'rewards': copy.deepcopy(episode_rewards),\n",
    "        'distances': copy.deepcopy(episode_distances),\n",
    "        'component_weights': copy.deepcopy(component_weights),\n",
    "        'reward_changes': [],\n",
    "        'environment_changes': [episodes]  # The change happens right after the initial training\n",
    "    }\n",
    "    \n",
    "    # Branch 1: Adaptive branch - with reward function updates\n",
    "    print(f\"\\nStarting adaptive branch with leg length change to {changed_leg_length}...\")\n",
    "    \n",
    "    # Reset environment and agent\n",
    "    env = gym.make('BipedalWalker-v3', hardcore=False, render_mode=None)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.reset(seed=seed+1)  # Slightly different seed to prevent identical runs\n",
    "    \n",
    "    env = CustomBipedalWalkerEnv(env, numComponents=2, discretize_bins=discretize_bins)\n",
    "    \n",
    "    # Set changed parameters\n",
    "    env.setEnvironmentParameters(\n",
    "        leg_length=changed_leg_length,  # Change to long legs\n",
    "        terrain_roughness=0.5,\n",
    "        gravity=9.8\n",
    "    )\n",
    "    \n",
    "    # Reset agent\n",
    "    agent = DQLearningAgent(\n",
    "        env=env, \n",
    "        stateSize=state_size, \n",
    "        actionSize=action_size, \n",
    "        device=device,\n",
    "        learningRate=0.0005,\n",
    "        discountFactor=0.99,\n",
    "        epsilon=epsilon,  # Continue with the same exploration rate\n",
    "        epsilonDecay=0.9995,\n",
    "        epsilonMin=0.05,\n",
    "        replayBufferSize=100000,\n",
    "        batchSize=64,\n",
    "        targetUpdateFreq=200\n",
    "    )\n",
    "    \n",
    "    # Restore saved state\n",
    "    agent.model.load_state_dict(model_state)\n",
    "    agent.targetModel.load_state_dict(target_model_state)\n",
    "    agent.memory = memory\n",
    "    \n",
    "    # Restore reward components\n",
    "    for name, func in reward_components.items():\n",
    "        component_num = int(name[-1])\n",
    "        env.setComponentReward(component_num, func)\n",
    "    \n",
    "    if component_weights_save:\n",
    "        env.componentWeights = component_weights_save\n",
    "    \n",
    "    # Reset episode tracking for continuation\n",
    "    adaptive_episode_rewards = []\n",
    "    adaptive_episode_distances = []\n",
    "    adaptive_component_weights = []\n",
    "    adaptive_reward_changes = []\n",
    "    \n",
    "    def onAdaptiveEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "        nonlocal adaptive_episode_rewards, adaptive_episode_distances, adaptive_component_weights, adaptive_reward_changes\n",
    "        \n",
    "        # Record metrics\n",
    "        adaptive_episode_rewards.append(reward)\n",
    "        \n",
    "        # For BipedalWalker, track forward distance\n",
    "        if hasattr(env.env.unwrapped, 'hull'):\n",
    "            distance = env.env.unwrapped.hull.position[0]\n",
    "        else:\n",
    "            distance = 0  # Fallback\n",
    "        adaptive_episode_distances.append(distance)\n",
    "        \n",
    "        # Calculate metrics for decision making\n",
    "        metrics = {\n",
    "            'currentEpisode': episode,\n",
    "            'recentRewards': adaptive_episode_rewards[-100:] if len(adaptive_episode_rewards) > 100 else adaptive_episode_rewards,\n",
    "            'averageDistance': np.mean(adaptive_episode_distances[-100:]) if adaptive_episode_distances else 0,\n",
    "            'distanceVariance': np.var(adaptive_episode_distances[-100:]) if len(adaptive_episode_distances) > 1 else 0\n",
    "        }\n",
    "        \n",
    "        # Collect component weights\n",
    "        if hasattr(env, 'getCurrentWeights'):\n",
    "            weights = env.getCurrentWeights()\n",
    "            adaptive_component_weights.append({\n",
    "                'episode': episode,\n",
    "                'stability': weights['stability'],\n",
    "                'efficiency': weights['efficiency']\n",
    "            })\n",
    "        \n",
    "        # Print debug info periodically\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"\\nAdaptive Branch - Episode {episode}:\")\n",
    "            print(f\"Recent Average Reward: {np.mean(metrics['recentRewards']):.2f}\")\n",
    "            print(f\"Average Distance: {metrics['averageDistance']:.2f}\")\n",
    "            \n",
    "            if hasattr(env, 'getCurrentWeights'):\n",
    "                weights = env.getCurrentWeights()\n",
    "                print(f\"Component Weights - Stability: {weights['stability']:.2f}, \"\n",
    "                      f\"Efficiency: {weights['efficiency']:.2f}\")\n",
    "        \n",
    "        # Handle LLM updates for reward function\n",
    "        if updatesystem is not None and updatesystem.waitingTime('adaptive', metrics, updatesystem.lastUpdateEpisode):\n",
    "            for component in range(1, 3):\n",
    "                updatesystem.targetComponent = component\n",
    "                current_func = env.rewardComponents[f'rewardFunction{component}']\n",
    "                new_function, updated = updatesystem.validateAndUpdate(current_func)\n",
    "                \n",
    "                if updated:\n",
    "                    # Apply the update\n",
    "                    env.setComponentReward(component, new_function)\n",
    "                    adaptive_reward_changes.append(episode)\n",
    "                    updatesystem.lastUpdateEpisode = episode\n",
    "                    \n",
    "                    print(f\"✓ Adaptive Branch - LLM update for component {component} at episode {episode}\")\n",
    "    \n",
    "    # Train the adaptive branch\n",
    "    adaptive_agent, adaptive_env, _ = trainDQLearning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        numEpisodes=post_change_episodes,\n",
    "        updateSystem=update_system,\n",
    "        onEpisodeEnd=onAdaptiveEpisodeEnd\n",
    "    )\n",
    "    \n",
    "    # Branch 2: Static branch - without reward function updates\n",
    "    print(f\"\\nStarting static branch with leg length change to {changed_leg_length}...\")\n",
    "    \n",
    "    # Reset environment and agent\n",
    "    env = gym.make('BipedalWalker-v3', hardcore=False, render_mode=None)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.reset(seed=seed+2)  # Different seed to prevent identical runs\n",
    "    \n",
    "    env = CustomBipedalWalkerEnv(env, numComponents=2, discretize_bins=discretize_bins)\n",
    "    \n",
    "    # Set changed parameters\n",
    "    env.setEnvironmentParameters(\n",
    "        leg_length=changed_leg_length,  # Change to long legs\n",
    "        terrain_roughness=0.5,\n",
    "        gravity=9.8\n",
    "    )\n",
    "    \n",
    "    # Reset agent\n",
    "    agent = DQLearningAgent(\n",
    "        env=env, \n",
    "        stateSize=state_size, \n",
    "        actionSize=action_size, \n",
    "        device=device,\n",
    "        learningRate=0.0005,\n",
    "        discountFactor=0.99,\n",
    "        epsilon=epsilon,  # Continue with the same exploration rate\n",
    "        epsilonDecay=0.9995,\n",
    "        epsilonMin=0.05,\n",
    "        replayBufferSize=100000,\n",
    "        batchSize=64,\n",
    "        targetUpdateFreq=200\n",
    "    )\n",
    "    \n",
    "    # Restore saved state\n",
    "    agent.model.load_state_dict(model_state)\n",
    "    agent.targetModel.load_state_dict(target_model_state)\n",
    "    agent.memory = memory\n",
    "    \n",
    "    # Restore reward components\n",
    "    for name, func in reward_components.items():\n",
    "        component_num = int(name[-1])\n",
    "        env.setComponentReward(component_num, func)\n",
    "    \n",
    "    if component_weights_save:\n",
    "        env.componentWeights = component_weights_save\n",
    "    \n",
    "    # Create null update system that never updates\n",
    "    null_update_system = type('NullUpdateSystem', (), {'waitingTime': lambda *args: False, 'lastUpdateEpisode': 0, 'targetComponent': 1})()\n",
    "    \n",
    "    # Reset episode tracking for continuation\n",
    "    static_episode_rewards = []\n",
    "    static_episode_distances = []\n",
    "    static_component_weights = []\n",
    "    \n",
    "    def onStaticEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "        nonlocal static_episode_rewards, static_episode_distances, static_component_weights\n",
    "        \n",
    "        # Record metrics\n",
    "        static_episode_rewards.append(reward)\n",
    "        \n",
    "        # For BipedalWalker, track forward distance\n",
    "        if hasattr(env.env.unwrapped, 'hull'):\n",
    "            distance = env.env.unwrapped.hull.position[0]\n",
    "        else:\n",
    "            distance = 0  # Fallback\n",
    "        static_episode_distances.append(distance)\n",
    "        \n",
    "        # Collect component weights\n",
    "        if hasattr(env, 'getCurrentWeights'):\n",
    "            weights = env.getCurrentWeights()\n",
    "            static_component_weights.append({\n",
    "                'episode': episode,\n",
    "                'stability': weights['stability'],\n",
    "                'efficiency': weights['efficiency']\n",
    "            })\n",
    "        \n",
    "        # Print debug info periodically\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"\\nStatic Branch - Episode {episode}:\")\n",
    "            print(f\"Recent Average Reward: {np.mean(static_episode_rewards[-100:] if len(static_episode_rewards) >= 100 else static_episode_rewards):.2f}\")\n",
    "            print(f\"Average Distance: {np.mean(static_episode_distances[-100:] if len(static_episode_distances) >= 100 else static_episode_distances):.2f}\")\n",
    "            \n",
    "            if hasattr(env, 'getCurrentWeights'):\n",
    "                weights = env.getCurrentWeights()\n",
    "                print(f\"Component Weights - Stability: {weights['stability']:.2f}, \"\n",
    "                      f\"Efficiency: {weights['efficiency']:.2f}\")\n",
    "    \n",
    "    # Train the static branch\n",
    "    static_agent, static_env, _ = trainDQLearning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        numEpisodes=post_change_episodes,\n",
    "        updateSystem=null_update_system,\n",
    "        onEpisodeEnd=onStaticEpisodeEnd\n",
    "    )\n",
    "    \n",
    "    # Combine results\n",
    "    adaptive_results['rewards'].extend(adaptive_episode_rewards)\n",
    "    adaptive_results['distances'].extend(adaptive_episode_distances)\n",
    "    adaptive_results['component_weights'].extend(adaptive_component_weights)\n",
    "    adaptive_results['reward_changes'].extend([episodes + ep for ep in adaptive_reward_changes])\n",
    "    \n",
    "    static_results['rewards'].extend(static_episode_rewards)\n",
    "    static_results['distances'].extend(static_episode_distances)\n",
    "    static_results['component_weights'].extend(static_component_weights)\n",
    "    \n",
    "    # Store combined results\n",
    "    results = {\n",
    "        'adaptive': adaptive_results,\n",
    "        'static': static_results,\n",
    "        'change_episode': episodes,\n",
    "        'total_episodes': episodes + post_change_episodes,\n",
    "        'leg_lengths': {\n",
    "            'initial': initial_leg_length,\n",
    "            'changed': changed_leg_length\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print final comparison\n",
    "    print(f\"\\nComparison after {post_change_episodes} episodes post-change:\")\n",
    "    print(f\"Adaptive Branch - Final average reward: {np.mean(adaptive_episode_rewards[-100:]):.2f}\")\n",
    "    print(f\"Adaptive Branch - Final average distance: {np.mean(adaptive_episode_distances[-100:]):.2f}\")\n",
    "    print(f\"Static Branch - Final average reward: {np.mean(static_episode_rewards[-100:]):.2f}\")\n",
    "    print(f\"Static Branch - Final average distance: {np.mean(static_episode_distances[-100:]):.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e078b7-f1d9-4f17-b3b3-a2a76760ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bipedal_results(results):\n",
    "    \"\"\"Analyze BipedalWalker robustness test results\"\"\"\n",
    "    # Extract data\n",
    "    rewards = results['adaptivereward']['rewards']\n",
    "    distances = results['adaptivereward']['distances']\n",
    "    reward_changes = results['adaptivereward']['rewardChanges']\n",
    "    environment_changes = results['adaptivereward']['environmentChanges']\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    window_size = 20\n",
    "    rewards_ma = pd.Series(rewards).rolling(window=window_size).mean()\n",
    "    distances_ma = pd.Series(distances).rolling(window=window_size).mean()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "    \n",
    "    # Plot rewards\n",
    "    ax1.plot(rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "    ax1.plot(rewards_ma, linewidth=2, color='darkblue', label=f'{window_size}-Episode Moving Average')\n",
    "    \n",
    "    # Add vertical lines for reward function updates\n",
    "    for episode in reward_changes:\n",
    "        ax1.axvline(x=episode, color='green', linestyle='--', alpha=0.7)\n",
    "        ax1.annotate(f\"Reward Update\", xy=(episode, ax1.get_ylim()[1]*0.9),\n",
    "                    xytext=(0, -20), textcoords='offset points',\n",
    "                    ha='center', va='bottom',\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    # Add vertical lines for environment changes\n",
    "    for episode in environment_changes:\n",
    "        ax1.axvline(x=episode, color='red', linestyle='--', alpha=0.7)\n",
    "        ax1.annotate(f\"Leg Length Change\", xy=(episode, ax1.get_ylim()[1]*0.7),\n",
    "                    xytext=(0, -20), textcoords='offset points',\n",
    "                    ha='center', va='bottom',\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    ax1.set_title(\"Reward Over Episodes\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Reward\", fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    # Plot distances\n",
    "    ax2.plot(distances, alpha=0.3, color='purple', label='Distance Traveled')\n",
    "    ax2.plot(distances_ma, linewidth=2, color='darkmagenta', label=f'{window_size}-Episode Moving Average')\n",
    "    \n",
    "    # Add vertical lines for reward function updates\n",
    "    for episode in reward_changes:\n",
    "        ax2.axvline(x=episode, color='green', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add vertical lines for environment changes\n",
    "    for episode in environment_changes:\n",
    "        ax2.axvline(x=episode, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax2.set_title(\"Distance Over Episodes\", fontsize=14)\n",
    "    ax2.set_xlabel(\"Episode\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Distance (m)\", fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot(fig, \"bipedal_performance\")\n",
    "    \n",
    "    # Create component weights visualization if available\n",
    "    if 'component_weights' in results['adaptivereward'] and results['adaptivereward']['component_weights']:\n",
    "        component_data = results['adaptivereward']['component_weights']\n",
    "        episodes = [d['episode'] for d in component_data]\n",
    "        stability_weights = [d['stability'] for d in component_data]\n",
    "        efficiency_weights = [d['efficiency'] for d in component_data]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.plot(episodes, stability_weights, 'b-', label='Stability Weight')\n",
    "        ax.plot(episodes, efficiency_weights, 'g-', label='Efficiency Weight')\n",
    "        \n",
    "        # Add vertical lines for reward function updates\n",
    "        for ep in reward_changes:\n",
    "            ax.axvline(x=ep, color='g', linestyle='--', alpha=0.5, \n",
    "                     label='Reward Update' if reward_changes.index(ep) == 0 else None)\n",
    "        \n",
    "        # Add vertical lines for environment changes\n",
    "        for ep in environment_changes:\n",
    "            ax.axvline(x=ep, color='r', linestyle='--', alpha=0.5,\n",
    "                     label='Environment Change' if environment_changes.index(ep) == 0 else None)\n",
    "        \n",
    "        ax.set_title('Evolution of Component Weights', fontsize=14)\n",
    "        ax.set_xlabel('Episode', fontsize=12)\n",
    "        ax.set_ylabel('Component Weight', fontsize=12)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_plot(fig, \"bipedal_component_weights\")\n",
    "    \n",
    "    # Calculate and print robustness metrics\n",
    "    print(\"\\n=== BipedalWalker Robustness Analysis ===\")\n",
    "    \n",
    "    # Recovery from bad initialization metrics\n",
    "    if 500 in reward_changes:\n",
    "        pre_change = slice(max(0, 500-100), 500)\n",
    "        post_change = slice(500, min(500+100, len(rewards)))\n",
    "        \n",
    "        pre_reward = np.mean(rewards[pre_change])\n",
    "        post_reward = np.mean(rewards[post_change])\n",
    "        \n",
    "        pre_distance = np.mean(distances[pre_change])\n",
    "        post_distance = np.mean(distances[post_change])\n",
    "        \n",
    "        # Calculate recovery time\n",
    "        recovery_threshold = 0.9 * pre_reward if pre_reward > 0 else 2.0\n",
    "        recovery_episode = None\n",
    "        \n",
    "        for i in range(500, min(500+500, len(rewards))):\n",
    "            if i + 10 < len(rewards) and np.mean(rewards[i-10:i+1]) >= recovery_threshold:\n",
    "                recovery_episode = i\n",
    "                break\n",
    "        \n",
    "        recovery_time = recovery_episode - 500 if recovery_episode else \"Did not recover\"\n",
    "        \n",
    "        print(\"\\nRecovery from Bad Initialization:\")\n",
    "        print(f\"Pre-change average reward: {pre_reward:.2f}\")\n",
    "        print(f\"Post-change average reward: {post_reward:.2f}\")\n",
    "        print(f\"Reward improvement: {((post_reward/pre_reward)-1)*100:.2f}%\" if pre_reward != 0 else \"N/A\")\n",
    "        print(f\"Pre-change average distance: {pre_distance:.2f}\")\n",
    "        print(f\"Post-change average distance: {post_distance:.2f}\")\n",
    "        print(f\"Distance improvement: {((post_distance/pre_distance)-1)*100:.2f}%\" if pre_distance != 0 else \"N/A\")\n",
    "        print(f\"Recovery time: {recovery_time} episodes\")\n",
    "    \n",
    "    # Environment change impact metrics\n",
    "    print(\"\\nResponse to Environment Changes:\")\n",
    "    for i, change_ep in enumerate(environment_changes):\n",
    "        if change_ep + 100 >= len(rewards):\n",
    "            continue\n",
    "            \n",
    "        pre_change = slice(max(0, change_ep-100), change_ep)\n",
    "        post_change = slice(change_ep, min(change_ep+100, len(rewards)))\n",
    "        \n",
    "        pre_reward = np.mean(rewards[pre_change])\n",
    "        post_reward = np.mean(rewards[post_change])\n",
    "        \n",
    "        pre_distance = np.mean(distances[pre_change])\n",
    "        post_distance = np.mean(distances[post_change])\n",
    "        \n",
    "        # Determine leg length\n",
    "        leg_desc = \"Long legs\" if i % 2 == 0 else \"Short legs\"\n",
    "        \n",
    "        print(f\"\\nChange {i+1} ({leg_desc}):\")\n",
    "        print(f\"Pre-change average reward: {pre_reward:.2f}\")\n",
    "        print(f\"Post-change average reward: {post_reward:.2f}\")\n",
    "        print(f\"Reward change: {((post_reward/pre_reward)-1)*100:.2f}%\" if pre_reward != 0 else \"N/A\")\n",
    "        print(f\"Pre-change average distance: {pre_distance:.2f}\")\n",
    "        print(f\"Post-change average distance: {post_distance:.2f}\")\n",
    "        print(f\"Distance change: {((post_distance/pre_distance)-1)*100:.2f}%\" if pre_distance != 0 else \"N/A\")\n",
    "        \n",
    "        # Calculate recovery time\n",
    "        recovery_threshold = 0.9 * pre_reward\n",
    "        recovery_episode = None\n",
    "        \n",
    "        for ep in range(change_ep, min(change_ep + 500, len(rewards))):\n",
    "            if np.mean(rewards[max(0, ep-10):ep+1]) >= recovery_threshold:\n",
    "                recovery_episode = ep\n",
    "                break\n",
    "        \n",
    "        if recovery_episode:\n",
    "            recovery_time = recovery_episode - change_ep\n",
    "            print(f\"Recovery time: {recovery_time} episodes\")\n",
    "        else:\n",
    "            print(\"Recovery time: Did not fully recover within 500 episodes\")\n",
    "    \n",
    "    # Calculate overall performance stability\n",
    "    min_reward = min(rewards[500:]) if len(rewards) > 500 else min(rewards)\n",
    "    avg_reward = np.mean(rewards[500:]) if len(rewards) > 500 else np.mean(rewards)\n",
    "    stability = min_reward / avg_reward if avg_reward != 0 else 0\n",
    "    \n",
    "    print(f\"\\nOverall Performance Stability: {stability:.3f}\")\n",
    "    print(f\"(Ratio of minimum to average reward, higher is better)\")\n",
    "    \n",
    "    return {\n",
    "        'overall_stability': stability,\n",
    "        'bad_init_recovery': ((post_reward/pre_reward)-1)*100 if 'post_reward' in locals() and pre_reward != 0 else None,\n",
    "        'avg_reward': avg_reward\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56d8ac-b2fd-4f3f-8e1e-20c995918228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCombinedResults(results, save_path=\"BipedalWalkerRobustnessResults\"):\n",
    "    \"\"\"\n",
    "    Plot combined results from the adaptive and static branches.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing results from both branches\n",
    "        save_path: Directory to save visualization outputs\n",
    "        \n",
    "    Returns:\n",
    "        dict: Performance advantage metrics\n",
    "    \"\"\"\n",
    "    # Extract data\n",
    "    adaptive_rewards = results['adaptive']['rewards']\n",
    "    adaptive_distances = results['adaptive']['distances']\n",
    "    static_rewards = results['static']['rewards']\n",
    "    static_distances = results['static']['distances']\n",
    "    \n",
    "    change_episode = results['change_episode']\n",
    "    reward_changes = results['adaptive']['reward_changes']\n",
    "    \n",
    "    # Calculate window size based on number of episodes\n",
    "    window_size = max(20, min(50, int(len(adaptive_rewards) / 100)))\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    adaptive_rewards_ma = pd.Series(adaptive_rewards).rolling(window=window_size).mean()\n",
    "    adaptive_distances_ma = pd.Series(adaptive_distances).rolling(window=window_size).mean()\n",
    "    static_rewards_ma = pd.Series(static_rewards).rolling(window=window_size).mean()\n",
    "    static_distances_ma = pd.Series(static_distances).rolling(window=window_size).mean()\n",
    "    \n",
    "    # Create visualization with subplots for rewards and distances\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "    \n",
    "    # Plot rewards\n",
    "    ax1.plot(adaptive_rewards_ma, linewidth=2, color='blue', label='Adaptive Reward')\n",
    "    ax1.plot(static_rewards_ma, linewidth=2, color='red', label='Static Reward')\n",
    "    \n",
    "    # Add vertical line for environment change\n",
    "    ax1.axvline(x=change_episode, color='black', linestyle='--', alpha=0.7, label='Environment Change')\n",
    "    \n",
    "    # Add vertical lines for reward function updates\n",
    "    for episode in reward_changes:\n",
    "        ax1.axvline(x=episode, color='green', linestyle='--', alpha=0.7, label='Reward Update' if episode == reward_changes[0] else None)\n",
    "    \n",
    "    # Customize reward plot\n",
    "    ax1.set_title(\"Reward Comparison Between Adaptive and Static Approaches\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Reward\", fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    # Add annotation for leg length change\n",
    "    leg_lengths = results['leg_lengths']\n",
    "    ax1.annotate(f\"Leg Length: {leg_lengths['initial']} → {leg_lengths['changed']}\", \n",
    "                xy=(change_episode, ax1.get_ylim()[1]*0.9),\n",
    "                xytext=(10, 0), textcoords='offset points',\n",
    "                ha='left', va='center',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', fc='yellow', alpha=0.3))\n",
    "    \n",
    "    # Plot distances\n",
    "    ax2.plot(adaptive_distances_ma, linewidth=2, color='blue', label='Adaptive Reward')\n",
    "    ax2.plot(static_distances_ma, linewidth=2, color='red', label='Static Reward')\n",
    "    \n",
    "    # Add vertical line for environment change\n",
    "    ax2.axvline(x=change_episode, color='black', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add vertical lines for reward function updates\n",
    "    for episode in reward_changes:\n",
    "        ax2.axvline(x=episode, color='green', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Customize distance plot\n",
    "    ax2.set_title(\"Distance Traveled Comparison\", fontsize=14)\n",
    "    ax2.set_xlabel(\"Episode\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Distance (m)\", fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot(fig, \"adaptive_static_comparison\", folder=save_path)\n",
    "    \n",
    "    # Create additional plot to highlight the immediate post-change period\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "    \n",
    "    # Calculate range to display (200 episodes before and 500 after change)\n",
    "    pre_change_window = 200\n",
    "    post_change_window = 500\n",
    "    start_idx = max(0, change_episode - pre_change_window)\n",
    "    end_idx = min(len(adaptive_rewards), change_episode + post_change_window)\n",
    "    \n",
    "    x_range = range(start_idx, end_idx)\n",
    "    \n",
    "    # Plot rewards for the zoomed period\n",
    "    ax1.plot(x_range, adaptive_rewards_ma.iloc[start_idx:end_idx], linewidth=2, color='blue', label='Adaptive Reward')\n",
    "    ax1.plot(x_range, static_rewards_ma.iloc[start_idx:end_idx], linewidth=2, color='red', label='Static Reward')\n",
    "    \n",
    "    # Add vertical line for environment change\n",
    "    ax1.axvline(x=change_episode, color='black', linestyle='--', alpha=0.7, label='Environment Change')\n",
    "    \n",
    "    # Add vertical lines for reward function updates within range\n",
    "    for episode in [ep for ep in reward_changes if start_idx <= ep <= end_idx]:\n",
    "        ax1.axvline(x=episode, color='green', linestyle='--', alpha=0.7, label='Reward Update' if episode == reward_changes[0] else None)\n",
    "    \n",
    "    # Add shaded regions for pre-post comparison windows\n",
    "    pre_change_start = max(start_idx, change_episode - 20)\n",
    "    pre_change_end = change_episode\n",
    "    post_change_start = change_episode\n",
    "    post_change_end = min(end_idx, change_episode + 100)\n",
    "    \n",
    "    # Pre-change window\n",
    "    ax1.axvspan(pre_change_start, pre_change_end, alpha=0.2, color='gray', label='Pre-Change Window')\n",
    "    # Post-change window\n",
    "    ax1.axvspan(post_change_start, post_change_end, alpha=0.2, color='yellow', label='Post-Change Window')\n",
    "    \n",
    "    # Customize reward plot\n",
    "    ax1.set_title(\"Zoomed View of Performance During Environmental Change\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Reward\", fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    # Plot distances for the zoomed period\n",
    "    ax2.plot(x_range, adaptive_distances_ma.iloc[start_idx:end_idx], linewidth=2, color='blue', label='Adaptive Reward')\n",
    "    ax2.plot(x_range, static_distances_ma.iloc[start_idx:end_idx], linewidth=2, color='red', label='Static Reward')\n",
    "    \n",
    "    # Add vertical line for environment change\n",
    "    ax2.axvline(x=change_episode, color='black', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add vertical lines for reward function updates within range\n",
    "    for episode in [ep for ep in reward_changes if start_idx <= ep <= end_idx]:\n",
    "        ax2.axvline(x=episode, color='green', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add shaded regions for pre-post comparison windows\n",
    "    ax2.axvspan(pre_change_start, pre_change_end, alpha=0.2, color='gray')\n",
    "    ax2.axvspan(post_change_start, post_change_end, alpha=0.2, color='yellow')\n",
    "    \n",
    "    # Customize distance plot\n",
    "    ax2.set_title(\"Zoomed View of Distance Traveled\", fontsize=14)\n",
    "    ax2.set_xlabel(\"Episode\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Distance (m)\", fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot(fig, \"adaptive_static_comparison_zoomed\", folder=save_path)\n",
    "    \n",
    "    # Calculate performance advantage metrics\n",
    "    advantage_metrics = calculateAdaptationMetrics(results)\n",
    "    \n",
    "    return advantage_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b21df4-d08c-47a0-a054-247945df3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAdaptationMetrics(results):\n",
    "    \"\"\"\n",
    "    Calculate detailed performance metrics comparing adaptive and static approaches.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing results from both branches\n",
    "        \n",
    "    Returns:\n",
    "        dict: Detailed performance metrics\n",
    "    \"\"\"\n",
    "    # Extract data\n",
    "    adaptive_rewards = results['adaptive']['rewards']\n",
    "    adaptive_distances = results['adaptive']['distances']\n",
    "    static_rewards = results['static']['rewards']\n",
    "    static_distances = results['static']['distances']\n",
    "    \n",
    "    change_episode = results['change_episode']\n",
    "    \n",
    "    # Define windows for analysis\n",
    "    pre_window = 20  # 20 episodes before change\n",
    "    immediate_window = 20  # 20 episodes immediately after change\n",
    "    recovery_window = 100  # Look ahead up to 100 episodes for recovery\n",
    "    \n",
    "    # Calculate pre-change performance (baseline)\n",
    "    pre_change_start = max(0, change_episode - pre_window)\n",
    "    pre_change_adaptive_rewards = adaptive_rewards[pre_change_start:change_episode]\n",
    "    pre_change_static_rewards = static_rewards[pre_change_start:change_episode]\n",
    "    pre_change_adaptive_distances = adaptive_distances[pre_change_start:change_episode]\n",
    "    pre_change_static_distances = static_distances[pre_change_start:change_episode]\n",
    "    \n",
    "    pre_change_adaptive_reward = np.mean(pre_change_adaptive_rewards)\n",
    "    pre_change_static_reward = np.mean(pre_change_static_rewards)\n",
    "    pre_change_adaptive_distance = np.mean(pre_change_adaptive_distances)\n",
    "    pre_change_static_distance = np.mean(pre_change_static_distances)\n",
    "    \n",
    "    # Calculate immediate post-change performance\n",
    "    post_change_end = min(len(adaptive_rewards), change_episode + immediate_window)\n",
    "    immediate_adaptive_rewards = adaptive_rewards[change_episode:post_change_end]\n",
    "    immediate_static_rewards = static_rewards[change_episode:post_change_end]\n",
    "    immediate_adaptive_distances = adaptive_distances[change_episode:post_change_end]\n",
    "    immediate_static_distances = static_distances[change_episode:post_change_end]\n",
    "    \n",
    "    immediate_adaptive_reward = np.mean(immediate_adaptive_rewards)\n",
    "    immediate_static_reward = np.mean(immediate_static_rewards)\n",
    "    immediate_adaptive_distance = np.mean(immediate_adaptive_distances)\n",
    "    immediate_static_distance = np.mean(immediate_static_distances)\n",
    "    \n",
    "    # Calculate performance drops\n",
    "    adaptive_reward_drop_pct = ((pre_change_adaptive_reward - immediate_adaptive_reward) / pre_change_adaptive_reward * 100) if pre_change_adaptive_reward > 0 else 0\n",
    "    static_reward_drop_pct = ((pre_change_static_reward - immediate_static_reward) / pre_change_static_reward * 100) if pre_change_static_reward > 0 else 0\n",
    "    adaptive_distance_drop_pct = ((pre_change_adaptive_distance - immediate_adaptive_distance) / pre_change_adaptive_distance * 100) if pre_change_adaptive_distance > 0 else 0\n",
    "    static_distance_drop_pct = ((pre_change_static_distance - immediate_static_distance) / pre_change_static_distance * 100) if pre_change_static_distance > 0 else 0\n",
    "    \n",
    "    # Calculate recovery times (episodes to reach 90% of pre-change performance)\n",
    "    adaptive_recovery_threshold = 0.9 * pre_change_adaptive_reward\n",
    "    static_recovery_threshold = 0.9 * pre_change_static_reward\n",
    "    \n",
    "    adaptive_recovery_episode = None\n",
    "    static_recovery_episode = None\n",
    "    \n",
    "    # Use rolling window to smooth performance for recovery detection\n",
    "    window_size = 10\n",
    "    \n",
    "    for i in range(change_episode, min(change_episode + recovery_window, len(adaptive_rewards))):\n",
    "        # Need at least window_size episodes after i to calculate rolling average\n",
    "        if i + window_size >= len(adaptive_rewards):\n",
    "            break\n",
    "            \n",
    "        adaptive_window_avg = np.mean(adaptive_rewards[i:i+window_size])\n",
    "        static_window_avg = np.mean(static_rewards[i:i+window_size])\n",
    "        \n",
    "        if adaptive_recovery_episode is None and adaptive_window_avg >= adaptive_recovery_threshold:\n",
    "            adaptive_recovery_episode = i\n",
    "        \n",
    "        if static_recovery_episode is None and static_window_avg >= static_recovery_threshold:\n",
    "            static_recovery_episode = i\n",
    "    \n",
    "    adaptive_recovery_time = (adaptive_recovery_episode - change_episode) if adaptive_recovery_episode else \"Did not recover\"\n",
    "    static_recovery_time = (static_recovery_episode - change_episode) if static_recovery_episode else \"Did not recover\"\n",
    "    \n",
    "    # Look at final performance (last 100 episodes)\n",
    "    final_window = 100\n",
    "    final_adaptive_reward = np.mean(adaptive_rewards[-final_window:])\n",
    "    final_static_reward = np.mean(static_rewards[-final_window:])\n",
    "    final_adaptive_distance = np.mean(adaptive_distances[-final_window:])\n",
    "    final_static_distance = np.mean(static_distances[-final_window:])\n",
    "    \n",
    "    # Calculate performance advantage\n",
    "    reward_advantage_pct = ((final_adaptive_reward / final_static_reward) - 1) * 100 if final_static_reward > 0 else 0\n",
    "    distance_advantage_pct = ((final_adaptive_distance / final_static_distance) - 1) * 100 if final_static_distance > 0 else 0\n",
    "    \n",
    "    # Track adaptive reward updates\n",
    "    reward_updates = results['adaptive']['reward_changes']\n",
    "    num_updates = len([ep for ep in reward_updates if ep > change_episode])\n",
    "    first_update = min([ep for ep in reward_updates if ep > change_episode], default=None)\n",
    "    update_delay = (first_update - change_episode) if first_update else None\n",
    "    \n",
    "    # Compile all metrics\n",
    "    metrics = {\n",
    "        'pre_change': {\n",
    "            'adaptive_reward': pre_change_adaptive_reward,\n",
    "            'static_reward': pre_change_static_reward,\n",
    "            'adaptive_distance': pre_change_adaptive_distance,\n",
    "            'static_distance': pre_change_static_distance\n",
    "        },\n",
    "        'immediate_post_change': {\n",
    "            'adaptive_reward': immediate_adaptive_reward,\n",
    "            'static_reward': immediate_static_reward,\n",
    "            'adaptive_distance': immediate_adaptive_distance,\n",
    "            'static_distance': immediate_static_distance\n",
    "        },\n",
    "        'performance_drop_pct': {\n",
    "            'adaptive_reward': adaptive_reward_drop_pct,\n",
    "            'static_reward': static_reward_drop_pct,\n",
    "            'adaptive_distance': adaptive_distance_drop_pct,\n",
    "            'static_distance': static_distance_drop_pct\n",
    "        },\n",
    "        'recovery': {\n",
    "            'adaptive_episodes': adaptive_recovery_time,\n",
    "            'static_episodes': static_recovery_time\n",
    "        },\n",
    "        'final_performance': {\n",
    "            'adaptive_reward': final_adaptive_reward,\n",
    "            'static_reward': final_static_reward,\n",
    "            'adaptive_distance': final_adaptive_distance,\n",
    "            'static_distance': final_static_distance\n",
    "        },\n",
    "        'performance_advantage_pct': {\n",
    "            'reward': reward_advantage_pct,\n",
    "            'distance': distance_advantage_pct\n",
    "        },\n",
    "        'updates': {\n",
    "            'count': num_updates,\n",
    "            'first_update_episode': first_update,\n",
    "            'update_delay': update_delay\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n=== Adaptation Performance Metrics ===\\n\")\n",
    "    \n",
    "    print(\"Performance Drop After Environmental Change:\")\n",
    "    print(f\"Adaptive Reward: {adaptive_reward_drop_pct:.1f}%\")\n",
    "    print(f\"Static Reward: {static_reward_drop_pct:.1f}%\")\n",
    "    print(f\"Difference: {(static_reward_drop_pct - adaptive_reward_drop_pct):.1f} percentage points\")\n",
    "    \n",
    "    print(\"\\nRecovery Time:\")\n",
    "    print(f\"Adaptive Reward: {adaptive_recovery_time if isinstance(adaptive_recovery_time, str) else f'{adaptive_recovery_time} episodes'}\")\n",
    "    print(f\"Static Reward: {static_recovery_time if isinstance(static_recovery_time, str) else f'{static_recovery_time} episodes'}\")\n",
    "    \n",
    "    print(\"\\nFinal Performance Advantage:\")\n",
    "    print(f\"Reward: Adaptive is {reward_advantage_pct:.1f}% higher\")\n",
    "    print(f\"Distance: Adaptive is {distance_advantage_pct:.1f}% higher\")\n",
    "    \n",
    "    print(\"\\nReward Function Updates:\")\n",
    "    print(f\"Number of updates after environmental change: {num_updates}\")\n",
    "    print(f\"First update occurred at episode: {first_update if first_update else 'N/A'}\")\n",
    "    print(f\"Delay between environmental change and first update: {update_delay if update_delay else 'N/A'} episodes\")\n",
    "    \n",
    "    # Create a table format suitable for inclusion in a paper\n",
    "    print(\"\\nTable for Paper:\")\n",
    "    headers = [\"Metric\", \"Adaptive Approach\", \"Static Approach\", \"Difference\"]\n",
    "    rows = [\n",
    "        [\"Performance Drop (%)\", f\"{adaptive_reward_drop_pct:.1f}%\", f\"{static_reward_drop_pct:.1f}%\", f\"{(static_reward_drop_pct - adaptive_reward_drop_pct):.1f} pp\"],\n",
    "        [\"Recovery Time (eps)\", f\"{adaptive_recovery_time if isinstance(adaptive_recovery_time, str) else adaptive_recovery_time}\", f\"{static_recovery_time if isinstance(static_recovery_time, str) else static_recovery_time}\", \n",
    "         f\"{(static_recovery_time - adaptive_recovery_time) if not isinstance(adaptive_recovery_time, str) and not isinstance(static_recovery_time, str) else 'N/A'}\"],\n",
    "        [\"Final Reward\", f\"{final_adaptive_reward:.2f}\", f\"{final_static_reward:.2f}\", f\"{reward_advantage_pct:.1f}%\"],\n",
    "        [\"Final Distance\", f\"{final_adaptive_distance:.2f}\", f\"{final_static_distance:.2f}\", f\"{distance_advantage_pct:.1f}%\"]\n",
    "    ]\n",
    "    \n",
    "    # Print the table\n",
    "    print(f\"{headers[0]:20} {headers[1]:20} {headers[2]:20} {headers[3]:20}\")\n",
    "    print(\"-\" * 80)\n",
    "    for row in rows:\n",
    "        print(f\"{row[0]:20} {row[1]:20} {row[2]:20} {row[3]:20}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27ccae-7b22-4008-b262-11db3ce33880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Run the Reward Update Experiment without environmental changes\n",
    "print(\"Running Part 1: Reward Update Experiment without Environmental Changes\")\n",
    "update_results = runRewardUpdateExperiment(\n",
    "    episodes=10000,\n",
    "    update_interval=2000,\n",
    "    seed=42,\n",
    "    discretize_bins=3\n",
    ")\n",
    "\n",
    "# Part 2: Run the Comparative Experiment with Environmental Change\n",
    "print(\"\\nRunning Part 2: Adaptive vs Static Comparison with Environmental Change\")\n",
    "comparison_results = runAdaptationComparisonExperiment(\n",
    "    episodes=5000,            # Initial training episodes\n",
    "    change_episode=5000,      # When to make environmental change \n",
    "    post_change_episodes=2000, # Episodes to run after change\n",
    "    seed=43,\n",
    "    discretize_bins=3\n",
    ")\n",
    "\n",
    "# Visualize and analyze the comparative results\n",
    "print(\"\\nAnalyzing and Visualizing Comparison Results\")\n",
    "advantage_metrics = plotCombinedResults(\n",
    "    comparison_results,\n",
    "    save_path=\"BipedalWalkerRobustnessResults\"\n",
    ")\n",
    "\n",
    "print(\"\\nRobustness experiments completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ac020-d3cc-4f2b-970b-0119d06e7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the BipedalWalker robustness experiment\n",
    "results = runBipedalRobustnessTest(\n",
    "    episodes=10000,\n",
    "    update_interval=10000,\n",
    "    leg_changes=[50, 50],  # Alternate between short and long legs\n",
    "    terrain_roughness=1.0,\n",
    "    gravity=9.8,\n",
    "    seed=42,\n",
    "    discretize_bins=3  # Discretize continuous action space into 3 values per dimension\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "print(\"\\n--- Creating Performance Visualizations ---\")\n",
    "metrics = analyze_bipedal_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e5cbc-95f1-463f-80fc-a2fc58f184ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how component weights evolve with different leg lengths\n",
    "if 'component_weights' in results['adaptivereward'] and results['adaptivereward']['component_weights']:\n",
    "    # Extract component weight data\n",
    "    component_data = results['adaptivereward']['component_weights']\n",
    "    episodes = [d['episode'] for d in component_data]\n",
    "    stability_weights = [d['stability'] for d in component_data]\n",
    "    efficiency_weights = [d['efficiency'] for d in component_data]\n",
    "    \n",
    "    # Extract environment changes\n",
    "    env_changes = results['adaptivereward']['environmentChanges']\n",
    "    \n",
    "    # Define segments between environment changes\n",
    "    segments = []\n",
    "    for i in range(len(env_changes)):\n",
    "        start = env_changes[i-1] if i > 0 else 0\n",
    "        end = env_changes[i]\n",
    "        segments.append((start, end, \"Short legs\" if i % 2 == 0 else \"Long legs\"))\n",
    "    \n",
    "    # Add final segment\n",
    "    segments.append((env_changes[-1] if env_changes else 0, episodes[-1], \n",
    "                    \"Long legs\" if len(env_changes) % 2 == 0 else \"Short legs\"))\n",
    "    \n",
    "    # Calculate average weights per segment\n",
    "    segment_weights = []\n",
    "    for start, end, label in segments:\n",
    "        # Find weights in this segment\n",
    "        segment_indices = [i for i, ep in enumerate(episodes) if start <= ep < end]\n",
    "        if segment_indices:\n",
    "            avg_stability = np.mean([stability_weights[i] for i in segment_indices])\n",
    "            avg_efficiency = np.mean([efficiency_weights[i] for i in segment_indices])\n",
    "            segment_weights.append((label, avg_stability, avg_efficiency))\n",
    "    \n",
    "    # Create a bar chart showing component weight distribution\n",
    "    labels = [s[0] for s in segment_weights]\n",
    "    stability_avgs = [s[1] for s in segment_weights]\n",
    "    efficiency_avgs = [s[2] for s in segment_weights]\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    rects1 = ax.bar(x - width/2, stability_avgs, width, label='Stability Weight')\n",
    "    rects2 = ax.bar(x + width/2, efficiency_avgs, width, label='Efficiency Weight')\n",
    "    \n",
    "    ax.set_ylabel('Average Weight')\n",
    "    ax.set_title('Component Weight Distribution by Leg Length')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i, rect in enumerate(rects1):\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    for i, rect in enumerate(rects2):\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot(fig, \"bipedal_component_weight_distribution\")\n",
    "    \n",
    "    print(\"\\nComponent Weight Analysis by Leg Length:\")\n",
    "    for label, stability, efficiency in segment_weights:\n",
    "        print(f\"{label}: Stability = {stability:.3f}, Efficiency = {efficiency:.3f}\")\n",
    "        if stability > efficiency:\n",
    "            print(f\"  Dominant component: Stability (+{stability-efficiency:.3f})\")\n",
    "        else:\n",
    "            print(f\"  Dominant component: Efficiency (+{efficiency-stability:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
